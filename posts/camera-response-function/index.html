<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Camera Response Function | RoboAlgorithms</title>

<meta name="keywords" content="computer vision, python" />
<meta name="description" content="Explore the fundamentals of the camera response function with Python.">
<meta name="author" content="Martin Mihaylov">
<link rel="canonical" href="https://roboalgorithms.com/posts/camera-response-function/" />
<link href="/assets/css/stylesheet.min.808707055d7c4ea20e24a6572ea095a6ba18683798dbe0c73c50b16b53071d36.css" integrity="sha256-gIcHBV18TqIOJKZXLqCVproYaDeY2&#43;DHPFCxa1MHHTY=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="https://roboalgorithms.com/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://roboalgorithms.com/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://roboalgorithms.com/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://roboalgorithms.com/apple-touch-icon.png">
<link rel="mask-icon" href="https://roboalgorithms.com/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.79.0" />
<script>if(!sessionStorage.getItem("_swa")&&document.referrer.indexOf(location.protocol+"//"+location.host)!== 0){fetch("https://counter.dev/track?"+new URLSearchParams({referrer:document.referrer,screen:screen.width+"x"+screen.height,user:"roboalgorithms",utcoffset:"2"}))};sessionStorage.setItem("_swa","1");</script>
<meta property="og:title" content="Camera Response Function" />
<meta property="og:description" content="Explore the fundamentals of the camera response function with Python." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://roboalgorithms.com/posts/camera-response-function/" />
<meta property="og:image" content="https://roboalgorithms.com/crf/crf-cover.png" /><meta property="article:published_time" content="2021-02-27T17:00:00+01:00" />
<meta property="article:modified_time" content="2021-02-27T17:00:00+01:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://roboalgorithms.com/crf/crf-cover.png" />
<meta name="twitter:title" content="Camera Response Function"/>
<meta name="twitter:description" content="Explore the fundamentals of the camera response function with Python."/>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Camera Response Function",
  "name": "Camera Response Function",
  "description": "First principles exploration üëã The goal of this article is to explore the topic of the camera response function experimentally. We\u0026amp;rsquo;ll build up from first principles and the ‚Ä¶",
  "keywords": [
    "computer vision", "python"
  ],
  "articleBody": "First principles exploration üëã The goal of this article is to explore the topic of the camera response function experimentally. We‚Äôll build up from first principles and the main focus will be on building intuition with code examples. The only pre-requisite is base familiarity with Python + NumPy and, of course, curiosity about the topic. üßë‚ÄçüöÄ\nIf you‚Äôre looking for a quick way to find out the response function of your camera, you can check out the implementation of CalibrateDebevec in OpenCV, or stick around to get a deeper intuition.\nI‚Äôve made an effort to link to interesting resources that provide a good starting point to dig deeper into specific directions that could not be covered in this article. You‚Äôre encouraged to follow them and come back to this at your own pace.\nüëâ You can follow along and play with the code examples. The Jupyter notebook is available on Github. üìî  üëâ Subscribe to the RoboAlgorithms YT channel to be notified about the complementary video on the camera response function (coming out soon). üé•  What the CRF? ü§î A camera measures the light intensity hitting the imaging sensor and outputs an image with pixel values that in some way correspond to that light intensity. This correspondence is modeled by the camera response function (CRF).\nFor a computer vision engineer, the pixel values are the only observation of the real-world. Thus, it is helpful to understand how they relate to actual light intensities to correctly model that world.\nMost cameras have some non-linearity in their response to light intensities. Images taken by consumer-grade cameras are particularly affected by this because they are optimized for viewing experience and not for measurement accuracy.\nüëâ If we know the response function, we can resolve that ambiguity and turn any camera into a scientific measurement device! üî¨  Applications of the CRF üî® Many computer vision algorithms that rely on the brightness constancy assumption require accurate estimation of the CRF. For example, optical flow or any direct method for visual SLAM that works with pixel intensities such as DSO.\nDirect methods for visual SLAM are currently one of the most robust approaches for accurate robot localization in challenging environments where GNSS coverage is unreliable. An autonomous last-mile robot delivery service is an example that‚Äôs increasingly gaining commercial traction in recent years and may use such technology.\nüí° One could argue that due to the shift towards deep learning approaches and more readily available cameras that are optimized for machine vision, it will be unnecessary to manually calibrate for the camera response function in the future. Nevertheless, I still see value in exploring the fundamentals for learning purposes.  In this article, we‚Äôll look at the application of generating HDR images which also relies on knowledge about the camera response function because we need to ‚Äúfuse‚Äù multiple images taken at different exposure times.\nHow cameras measure light ‚òÄÔ∏è A camera is an array of photodiodes that converts incoming photons into electrical current or voltage. This phenomenon is known as the photoelectric effect and is the same phenomenon utilized in solar panels.\nüëâ We measure the voltage to determine the amount of incident light.  For each pixel, there‚Äôs a corresponding photodiode in the camera sensor. We take an image by exposing the sensor to light for a given period (exposure time). Since the photodiodes produce an electrical current proportional to the amount of light they receive, we can use that measurement to reconstruct the image.\nConverting voltages to pixel values üì∫ When we measure the voltage generated by the photodiode we get an analog quantity. But we need a digital value that fits into our pixel representation of 8 bits. A device that can perform this conversion is called an Analog-Digital (A/D) converter. Most A/D converters used in cameras today will output an 8, 12, or 16-bit digital value.\nAnalog-to-Digital Converter üìà We will start with an 8-bit A/D converter. Note that regardless of the bit depth, the range of possible values will be limited in the digital domain. Meaning that with 8-bit we only have  \\(2^{8} = 256\\)  , unique values to assign to all possible voltage measurements.\nLet‚Äôs simulate a naive A/D converter:\ndef max_value(bit_depth: int) - int: \"\"\"Return max integer value for the specified bit depth\"\"\" return 2**bit_depth - 1 def adc(milivolts: np.ndarray, bit_depth: int = 8) - int: \"\"\"Naive A/D converter milivolts -- accumulated voltage from a photodiode bit_depth -- target bit depth of the digitized output \"\"\" return np.clip(np.around(milivolts), 0, max_value(bit_depth)) def plot_adc_examples(): \"\"\"Plot a few interesting examples\"\"\" f, axes = plt.subplots(1, 3, figsize=(20, 5)) x_label = 'Input voltage (mV)' y_label = 'Output pixel value' voltages = np.arange(0, 10, 0.1) util.plot(axes[0], voltages, adc(voltages), \"In supported range\", x_label, y_label) voltages = np.arange(0, 0.5, 0.01) util.plot(axes[1], voltages, adc(voltages), \"Below supported range\", x_label, y_label) voltages = np.arange(255, 500, 1) util.plot(axes[2], voltages, adc(voltages), \"Above supported range\", x_label, y_label) plot_adc_examples() Quantization noise From the staircase-like plot above we can see the discrete jumps caused by the quantization effects. Because we‚Äôre losing some precision at this step, this effect also becomes a component of the total image noise known as the quantization noise.\nUnder- and overexposure The plot also demonstrates that any values below or above the supported range are irreversibly lost. This phenomenon is known as under- or overexposure. Our goal will always be to choose the exposure time such that this effect is minimized.\nDynamic Range The range of light intensities that the camera can capture without under- or overexposure is called the camera dynamic range. It is a very important metric for outdoor robot operation due to the large variations in light intensities.\nüí° Curiously, extending the dynamic range of the camera isn‚Äôt as simple as increasing the bit-depth of the A/D converter. The primary limiting factor is usually linked to the physical full-well capacity and noise level of the photodiodes. The bit-depth of the A/D converter is then selected to maximize that.  So our goal is to maximize the information we can get from every image. We have two ways to optimize for that:\n Choose a camera with a higher dynamic range Adjust the exposure time to minimize over- and underexposure  üëâ For very dimly lit scenarios, it is not always an option to adjust the exposure time because a very long exposure time can cause motion blur.  Programmable Gain Amplifier ‚¨õ Underexposure happens for dimly lit scenes or shorter exposure times when the photodiodes don‚Äôt have a chance to convert a sufficient amount of photons into electrical current. This is how a fully underexposed image looks:\nphotodiode_array_milivolts = np.array([0.028, 0.213, 0.234, 0.459]) pixel_values = adc(photodiode_array_milivolts) util.display_images([pixel_values], titles=[\"Underexposed image\"]) Cameras have a programmable-gain amplifier (PGA) that can amplify the voltages from the photodiodes and bring them in the range supported by the A/D converter. The level of amplification can be configured on most cameras through the ISO setting.\nüëâ We can capture images in the dark without increasing the exposure time as much.  def pga(milivolts: np.ndarray, gain: float = 1.0) - float: \"\"\"Naive programmable-gain amplifier milivolts -- input voltage from photodiode gain -- ratio output / input \"\"\" return gain * milivolts # Add an amplifier to the same example values from above pixel_values = adc(pga(photodiode_array_milivolts, gain=300.0)) util.display_images([pixel_values], titles=[\"Amplified image\"]) There‚Äôs a caveat: we did not model image noise. The weaker signal in dimply lit scenes results in a lower signal-to-noise ratio (SNR). In other words, amplifying the signal will also amplify the noise and that‚Äôs why using a higher gain usually results in grainier and lower quality images. In extreme cases when the strength of the signal is below the noise floor it can be difficult to extract any useful information.\nReducing the image noise can be performed in the digital image processing pipeline which is the final step before saving the image. This part significantly differs between camera models and may include white balance, tone mapping, gamma encoding, image compression and more.\nImage Acquisition Pipeline üì∏ The figure below provides an overview of the discussed components.\nüí° The camera response function is a single function that approximates the response of the whole system with all components inside. More on this later.  Further Recommended Reading üìö I‚Äôm aware that I‚Äôve touched upon a lot of topics very quickly. If you don‚Äôt have a good intuition already, I highly recommend reading this amazing interactive introduction on Cameras and Lenses. Seriously, please go check this article out.\nThe Science of Camera Sensors is another great resource in video form that provides an excellent introduction to the physics of how photodiodes and camera sensors work.\nAdvanced: For a more in-depth look into the theory and physics behind CMOS photodiodes, chapter 3 from this dissertation provides a good overview.\nSimulating a simple camera üì∑ Disclaimer ‚ÑπÔ∏è For the sake of brevity, many of the physical aspects of cameras are oversimplified in the presented examples. They are only meant to aid the intuition and shouldn‚Äôt be relied upon for accuracy or completeness. However, if you spot any mistakes, I would love to hear about them at üì® user=\"feedback\",domain=\"roboalgorithms.com\",document.write(user+\"@\"+domain);  feedback at roboalgorithms.com or on üê¶ twitter.\nAssumptions üìù üëâ The following assumptions provide the framework for the rest of the article. Feel free to skip this section and refer back to it later if you want to jump to the fun part already. Some parts may only become clear after completing the entire article.  Camera  uses the pinhole camera model ‚Äî i.e. no modeling of camera optics is monochrome ‚Äî i.e. no simulation of the color filter array and demosaicing 8-bit grayscale output format is used for the images static camera ‚Äî there‚Äôs no motion during or between image acquisitions  üí° Despite these simplifying assumptions, the concepts learned here are applicable to real-world RGB cameras with any kind of optical lens. Try to understand why.  Scene  receives constant light intensity from the light source (brightness constancy) static ‚Äî no moving objects in the scene during or between image acquisitions  In a practical sense, it means that we cannot accurately determine the camera response function on a partly cloudy day while relying on sunlight as our light source because the clouds may result in significant changes in the radiant flux received by the scene.\nüí° Beware that some artificial light sources might also flicker.  üëâ Given the above assumptions, if we take multiple images with the same exposure time t we must always get the same pixel values (not accounting for image noise).  Simulating a 2x2 pixel sensor üß± We will use the term pixel irradiance to refer to the number of photons hitting the physical area of a pixel per second ( \\(m^{‚àí2}‚ãÖs^{‚àí1}\\)  ). You can also refer to SI photon units definition on the wiki page of photon counting.\nimport numpy as np def get_sensor_irradiance(): \"\"\"Get a sample sensor irradiance We have an imaging sensor of 2x2 pixels (stored as 1D array) The sensor irradiance encodes how many photons are hitting the image sensor every second. The values are arbitrarily chosen. \"\"\" pix11_irradiance = 1122 # photons / second pix12_irradiance = 3242 # photons / second pix21_irradiance = 1452 # photons / second pix22_irradiance = 25031 # photons / second return np.array([ pix11_irradiance, pix12_irradiance, pix21_irradiance, pix22_irradiance]) Let‚Äôs start with the simple case of a linear camera response function. This intuitively means that doubling the amount of light (pixel irradiance) would result in doubling the observed pixel value. We define the CRF as:\n$$CRF = G(x) = a\\cdot x + b$$\nWe‚Äôre going to set  \\(a = 1\\)  and  \\(b = 0\\)  and assume that each photon increases the voltage by 1mV. This is easy to simulate but unfortunately, 100% detached from reality. Accurate simulation of that process is not in the scope of the article but feel free to enter that rabbit hole starting from quantum efficiency ;-). We also assume white Gaussian noise which approximates thermal noise. All other sources of noise will be ignored.\n$$CRF = G(x) = x$$\nTo get an estimate of the voltage accumulated in each pixel, we need to integrate the irradiance over the period of the exposure time t. We define taking an image as:\nüëâ $$I(k) = G(adc(pga(t\\cdot E(k))))$$  where E is the sensor irradiance (i.e. an array of all pixel irradiances), G is the camera response function as already defined above, t is the exposure time in seconds, I is the image intensity or observed pixel value and k is the index of the pixel.\nüí° In the literature, you may find the equation defined in its simpler form:\n$$I(k) = G^{*}(t\\cdot E(k))$$\nwhere the camera response function is:\n$$G^{*}(x) = G(adc(pga(x))$$\nI‚Äôve chosen to keep the functions separate here so that the logic for amplification and quantization is not intermingled with the CRF in the Python code.\n  Let‚Äôs code this:\nfrom typing import Callable ResponseFunction = Callable[[np.ndarray, int], np.ndarray] def capture_image(E: np.ndarray, t: float, G: ResponseFunction, bit_depth: int = 8, std_noise: float = 1.0) - np.ndarray: \"\"\"Simulate capturing an image with exposure time t. You can specify higher bit_depth for internal processing prior to finally converting the image to 8-bit depth before output. E -- sensor irradiance t -- exposure time (seconds) G -- camera response function (CRF) bit_depth -- bit depth for A/D converter output and response function input std_noise -- standard deviation for the Gaussian noise distribution \"\"\" # Non-realistic but simple calculation accumulated_milivolts = E * t # Add the white Gaussian noise noise = np.random.normal(0, std_noise, size=len(accumulated_milivolts)) accumulated_milivolts += noise image = G( adc(pga(accumulated_milivolts), bit_depth), bit_depth ) # Always convert to 8-bit depth prior to saving / outputting to_8bit_depth = max_value(8) / max_value(bit_depth) return np.around(image * to_8bit_depth) def linear_crf(X: np.ndarray, bit_depth: int) - np.ndarray: \"\"\"Linear camera response function G(x) = x\"\"\" return X Now take a couple of images with different exposures:\nimg1 = capture_image(E=get_sensor_irradiance(), t=1/10, G=linear_crf) img2 = capture_image(E=get_sensor_irradiance(), t=1/100, G=linear_crf) util.display_images([img1, img2], titles=[\"Exposure: t = 1/10 second\", \"Exposure: t = 1/100 second\"]) We can correctly expose all pixels with 1/100 second exposure time (none of the pixels are 0 or 255). It becomes more interesting if we have a scene with more extreme values which cannot be captured by the dynamic range of the camera sensor.\nThink of an image from inside a cave looking out.\nWe can simulate this by choosing more extreme values for the sensor irradiance:\ndef get_extreme_sensor_irradiance(): # Note that the range spans multiple orders of magnitude¬† return np.array([182, 5432, 15329, 252531]) t1, t2 = 1/5, 1/500 img1 = capture_image(E=get_extreme_sensor_irradiance(), t=t1, G=linear_crf) img2 = capture_image(E=get_extreme_sensor_irradiance(), t=t2, G=linear_crf) util.display_images([img1, img2], titles=[\"Exposure: t1 = 1/5 second\", \"Exposure: t2 = 1/500 second\"]) In this example, it is not possible to capture the scene without loss of information regardless of the chosen exposure time. Correctly exposing the brightest pixel(s) will result in underexposing the darkest pixel(s) and vice versa. Note that in the second image we have one underexposed and one overexposed pixel.\nFeel free to download the notebook and try it for yourself.\nHDR Images ü™Ç The pixel values provide us only partial information due to the limited dynamic range of the camera. To reconstruct an HDR image, we need to recover the full sensor irradiance.\nüí° Remember that outside our simulation, we can only observe the pixel values from the image and not the actual sensor irradiance.  üëâ We can do that by combining information from multiple images at different exposure times that allow us to expose each pixel correctly.  Inverse CRF üîô The inverse camera response function allows us to do precisely that. It defines the mapping from pixel values back to sensor irradiance or light intensity.\nWe define  \\(U := G^{-1}\\)  as the inverse of the CRF. Here we assume that G is monotonically increasing and thus invertible. That‚Äôs a fair assumption because cameras are mapping brighter scene regions to brighter image intensities.\nRemember that we defined taking an image as  \\(I(k) = G(t\\cdot E(k))\\)  . To go the other direction, we can apply the inverse function  \\(U(I(k)) = t\\cdot E(k)\\)  and we can get the sensor irradiance E from the image I if we know the exposure time t:\nüëâ $$E(k) = \\frac{U(I(k))}{t}$$  The inverse CRF is also identity function  \\(f(x) = x\\)  because:\n$$U(G(x)) = x$$\nRecover Image Irradiance üñºÔ∏è üí° I am using image irradiance to refer to the (partial) sensor irradiance recovered from a single image. This may differ from usage in the literature.  InverseResponseFunction = Callable[[np.ndarray], np.ndarray] def recover_image_irradiance(I: np.ndarray, t: float, U: InverseResponseFunction) - np.ndarray: \"\"\"Recover the irradiance from an image that was taken with known exposure time t. We cannot recover any useful information from oversaturated (255) or undersaturated (0) image intensities. Return 0 in both cases as this helps eliminate the need for separate case handling. I -- image (array of image intensities) t -- exposure time (seconds) U -- inverse of the response function \"\"\" I[I == 255] = 0 # oversaturated pixels return U(I) / t def linear_inverse_crf(I: np.ndarray) - np.ndarray: \"\"\"Linear inverse camera response function U(x) = x\"\"\" return I print(f\"\\nImage irradiance (t = {t1}):\") print(recover_image_irradiance(I=img1, t=t1, U=linear_inverse_crf)) print(f\"\\nImage irradiance (t = {t2}):\") print(recover_image_irradiance(I=img2, t=t2, U=linear_inverse_crf))   Image irradiance (t = 0.2): [185. 0. 0. 0.]\nImage irradiance (t = 0.002): [ 0. 6000. 15000. 0.]\n    Notice that we can recover the irradiance only for pixels that are not under- or overexposed. To recover the full sensor irradiance, we need to capture a series of images with different exposure times such that each pixel was correctly exposed at least once.\nLet‚Äôs take another image with a shorter exposure time:\nt3 = 1/2000 img3 = capture_image(E=get_extreme_sensor_irradiance(), t=t3, G=linear_crf) print(f\"\\nImage irradiance (t = {t3:.4f}):\") print(recover_image_irradiance(I=img3, t=t3, U=linear_inverse_crf))   Image irradiance (t = 0.0005): [ 0. 6000. 16000. 254000.]\n    Recover Sensor Irradiance üì∑ We can now estimate the full sensor irradiance by combining the individual image irradiances. Notice that the second and the third image have a conflicting estimate of the third pixel value (15000 vs 16000). We‚Äôre going to take the average.\nfrom typing import List def estimate_sensor_irradiance_average(images: List[np.ndarray], exposures: List[float], U: InverseResponseFunction) - np.ndarray: \"\"\"Recover the sensor irradiance images -- list of images to use for recovery exposures -- corresponding list of exposures U -- inverse response function \"\"\" assert len(images) == len(exposures), \\ \"The number of images and exposures don't match\" assert len(images)  0, \"Expected non-empty list of images\" image_size = images[0].shape[0] irradiance_sum = np.zeros(image_size) irradiance_count = np.zeros(image_size) for I, t in zip(images, exposures): image_irradiance = recover_image_irradiance(I, t, U) irradiance_sum += image_irradiance # Only non-zero values are observed and contribute to average irradiance_count[image_irradiance != 0] += 1 return np.divide(irradiance_sum, irradiance_count, out=np.zeros(image_size), where=irradiance_count != 0) Let‚Äôs compare that to the known ground-truth.\nestimated_sensor_irradiance = estimate_sensor_irradiance_average( [img1, img2, img3], [t1, t2, t3], U=linear_inverse_crf) util.print_error_to_ground_truth( estimated_sensor_irradiance, ground_truth=get_extreme_sensor_irradiance())   Ground-truth: [ 182 5432 15329 252531]\nEstimated: [ 180. 6750. 13750. 248000.]\nError (diff): [ 2. -1318. 1579. 4531.]\n    It seems we‚Äôre close but have lost some precision due to quantization effects and image noise. Can we do better? Maybe average over more images?\nImproving the sensor irradiance estimate In this section, we run several experiments to better understand how the noise affects the accuracy. First, we need a more representative dataset with more images.\nfrom typing import Tuple, List ImagesExposuresTuple = Tuple[List[np.ndarray], List[float]] def generate_sliding_exposure_dataset( E: np.ndarray, G: ResponseFunction, bit_depth: int = 8, start_us: int = 40, step_perc: int = 5, number_exposures: int = 220, images_per_exposure: int = 1) - ImagesExposuresTuple: \"\"\"Generate dataset with sliding exposure values E -- sensor irradiance G -- camera response function bit_depth -- bit depth for the camera's internal processing start_us -- microseconds for the starting value of the sequence step_perc -- percentage step between subsequent exposure values number_exposures -- number of unique exposure times to generate images_per_exposure -- number of images per exposure time \"\"\" def get_exposure_seconds(idx: int) - float: multiplier = (1 + (step_perc / 100)) ** idx return round(start_us * multiplier) * 1e-6 images, exposures = [], [] for exposure_idx in range(number_exposures): for _ in range(images_per_exposure): t = get_exposure_seconds(exposure_idx) images.append(capture_image(E=E, t=t, G=G)) exposures.append(t) return images, exposures Feel free to experiment with the parameters for the dataset generation and see what the impact on the final sensor irradiance estimates is.\nüëâ We can verify that the dataset is statistically well distributed by plotting the number of well-exposed observations for each of the pixels in the image.  images, exposures = generate_sliding_exposure_dataset( E=get_extreme_sensor_irradiance(), G=linear_crf) util.display_valid_observations(np.array(images)) Following, I define a helper function that will help us quickly run a few experiments.\nEstimateSensorIrradianceFunction = Callable[[List[np.ndarray], List[float], InverseResponseFunction], np.ndarray] def get_irradiance_estimates( estimate_func: EstimateSensorIrradianceFunction, images_per_exposure: int, number_experiments: int) - List[np.ndarray]: \"\"\"Get sensor irradiance estimates from a number of experiments\"\"\" estimates = [] for _ in range(number_experiments): images, exposures = generate_sliding_exposure_dataset( E=get_extreme_sensor_irradiance(), G=linear_crf, images_per_exposure=images_per_exposure) irradiance_estimate = estimate_func( images, exposures, U=linear_inverse_crf) estimates.append(irradiance_estimate) return estimates Let‚Äôs run 10 experiments with a single image per exposure time.\nirradiance_estimates = get_irradiance_estimates( estimate_sensor_irradiance_average, images_per_exposure=1, number_experiments=10) util.plot_errors_to_ground_truth( irradiance_estimates, ground_truth=get_extreme_sensor_irradiance())   RMSE: 1425.4632661240885\n    Now we have a baseline of root-mean-square error (RMSE) that we want to minimize. I‚Äôve also plotted the per-pixel RMSE and the per-experiment absolute error for the estimate.\nüëâ We can smoothen the noise variations between the experiments by averaging over multiple images with the same exposure.  rradiance_estimates = get_irradiance_estimates( estimate_sensor_irradiance_average, images_per_exposure=10,  number_experiments=10) util.plot_errors_to_ground_truth( irradiance_estimates, ground_truth=get_extreme_sensor_irradiance())   RMSE: 1215.9117041699783\n    The vertical oscillations have now subsided and the RMSE is slightly improved.\nNotice that the errors for most pixels, except the fourth (red in the figure), are biased toward negative values. This is not caused by the image noise because we would otherwise expect a mean error of zero.\nLet‚Äôs plot the distribution of the observations that we have for each pixel.\nutil.display_observations_distribution(np.array(images)) Now we see that the observations in our dataset are similarly skewed. The fourth pixel which had the smallest bias in the estimate also has the best observation distribution.\nThe skewed observations are the result of using a geometric sequence for the exposure times. In our case it looks like this:\nutil.plot_exposures(exposures) The geometric progression is convenient here due to the large range of values we need to cover. We are able to do so with just over 200 exposures. If we use an arithmetic progression, on the other hand, we would need significantly more pictures in the dataset which is not always practical or possible.\nInstead, we can improve the algorithm by adding a weighting function that discounts the contribution from observations with lower and higher pixel values.\nI‚Äôm going to use the weighting function proposed by Debevek \u0026 Malik:\n \\[w(z) = \\begin{cases} z - Z_{min} \u0026 \\text{for } z\\leq \\frac{1}{2}(Z_{min} + Z_{max})\\\\ Z_{max} - z \u0026 \\text{for } z\\gt \\frac{1}{2}(Z_{min} + Z_{max}) \\end{cases}\\]  where  \\(Z_{min}\\)  and  \\(Z_{max}\\)  are the least and greatest pixel values ‚Äî 0 and 255 respectively.\n \\[w(z) = \\begin{cases} z \u0026 \\text{for } z\\leq 127\\\\ 255 - z \u0026 \\text{for } z\\gt 127 \\end{cases}\\]  This function discounts both lower and higher pixel values. Besides the higher robustness towards different noise sources (see also CCD Saturation and Blooming), it also reflects the fact that values in the mid-range are most accurately encoded by non-linear camera response functions (more on this later).\ndef weighting_debevek_malik(pix_value: int) - int: \"\"\"Weighting function for recovering irradiance\"\"\" if pix_value  127: return pix_value if pix_value  127: return 255 - pix_value Let‚Äôs rewrite the function for sensor irradiance estimation to use the weighting function:\ndef estimate_sensor_irradiance_weighted( images: List[np.ndarray], exposures: List[float], U: Callable[[np.ndarray], np.ndarray], w: Callable[[int], int]) - np.ndarray: \"\"\"Estimate the sensor irradiance The recovered image irradiances are weighted according to the provided weighting function. This works best with higher number of images and well-distributed exposure times. images -- list of images to use for recovery exposures -- corresponding list of exposures U -- inverse response function w -- weighting function \"\"\" assert len(images) == len(exposures), \\ \"The number of images and exposures don't match\" assert len(images)  0, \"Expected non-empty list of images\" image_size = images[0].shape[0] w_vec = np.vectorize(w) irradiance_sum = np.zeros(image_size) irradiance_count = np.zeros(image_size) for I, t in zip(images, exposures): weights = w_vec(I) image_irradiance = recover_image_irradiance(I, t, U) irradiance_sum += weights * image_irradiance # Only non-zero values are observed and contribute to average irradiance_count[image_irradiance != 0] += weights[image_irradiance != 0] return np.divide(irradiance_sum, irradiance_count, out=np.zeros(image_size), where=irradiance_count != 0)   RMSE: 165.4355269685378\n    This change alone provides an almost 10x improvement in the estimate errors. I found out that using the square of the weighting function provides even better results.\ndef weighting_squared(pix_value: int) - int: return weighting_debevek_malik(pix_value) ** 2   RMSE: 72.49150623466912\n    And here‚Äôs the final estimate:\nimages, exposures = generate_sliding_exposure_dataset( E=get_extreme_sensor_irradiance(), G=linear_crf) recovered_sensor_irradiance = estimate_sensor_irradiance_weighted( images, exposures, U=linear_inverse_crf, w=weighting_squared) util.print_error_to_ground_truth( recovered_sensor_irradiance, ground_truth=get_extreme_sensor_irradiance())   Ground-truth: [ 182 5432 15329 252531]\nEstimated: [ 184.362 5436.691 15341.962 252564.875]\nError (diff): [ -2.362 -4.691 -12.962 -33.875]\n    Absolute vs Relative Irradiance ‚ö†Ô∏è üëâ So far we‚Äôve successfully recovered the relative sensor irradiance values.  Recovering the absolute sensor irradiance isn‚Äôt quite as straightforward for real cameras. We‚Äôve used a very simplified model of capturing images and haven‚Äôt taken the photodiode‚Äôs quantum efficiency and other physical aspects into consideration.\nAlthough it may seem that we fully recovered the irradiance values, this method can only recover the relative irradiance values up to an unknown constant factor  \\(X\\)  .\n$$ E_{absolute} = X \\cdot E_{relative} $$\nTo determine  \\(X\\)  , an absolute radiometric calibration can be performed. The process involves taking an image of a surface with known reflectance values in a controlled light environment.\nConveniently, the relative irradiance values that we obtained are sufficient for most applications, including HDR images.\nüëâ The irradiance values we recovered are our HDR image!  Displaying HDR Images üñ•Ô∏è We could display the HDR image by normalizing the values within the 0-255 range.\ndef normalize_255(image: np.ndarray) - np.ndarray: \"\"\"Normalize values withing range [0, 255]\"\"\" min_i, max_i = min(image), max(image) assert min_i  max_i, \"Range cannot be normalized (min == max)\" return np.around((image - min_i) * (255 / (max_i - min_i))) hdr_linear = normalize_255(recovered_sensor_irradiance) util.display_images([hdr_linear], titles=[\"HDR (linear)\"]) After normalization, the darkest and brightest pixels are pinned to the values of 0 and 255 respectively. All other values are linearly adjusted. The problem with this method is that usually the scene will have very few but extremely bright spots and this would cause the majority of the rest of the scene to appear underexposed.\nThe image above is a good example of this effect.\nThis is a limitation of the display technology today. Most devices still have a non-HDR display which is only capable of displaying 24-bit color which is exactly 8-bit per RGB channel. This sets a hard limit on the range of values [0-255].\nGamma encoding üìà To go around this problem cameras perform gamma encoding when saving images in 8-bit format. In doing so, the limited range can be utilized more efficiently.\ndef gamma(X: np.ndarray) - np.ndarray: assert all(X = 0) and all(X  1) return X**(1/2.2) def plot_gamma_function(): plt.title(\"Linear vs Gamma\") plt.xlabel(\"Input\") plt.ylabel(\"Output\") x = np.arange(0, 1, 0.001) plt.plot(x, gamma(x)) plt.plot(x, x) plot_gamma_function() The gamma function encodes lower values (darker tones) with significantly higher resolution. This mapping exploits the fact that our eyes perceive brightness proportionally to the logarithm of the light intensity. See Weber-Fechner law.\nThink of a light bulb with adjustable brightness. If we increase the light intensity by N two consecutive times, the perceived brightness change will be larger the first time.\nThe gamma encoding is reversed by a gamma decoding step which applies the inverse of the gamma function. Your monitor likely expects gamma encoded images and automatically does the decoding. All of this is formally defined in the sRGB color space.\nNote that the gamma function is just one of many possible tone mapping techniques. We are not going to cover more, but I highly recommend this great article on color spaces.\nFor very high dynamic range images you can also take the logarithm and normalize the resulting values over the [0-255] range. Following is an example of both approaches.\ndef apply_gamma(E: np.ndarray, bit_depth: int = 8): return gamma(E / max_value(bit_depth)) * max_value(bit_depth) hdr_gamma = apply_gamma(normalize_255(recovered_sensor_irradiance)) hdr_log = normalize_255(np.log(recovered_sensor_irradiance + 1)) util.display_images([hdr_linear, hdr_gamma, hdr_log], titles=[\"HDR (linear)\", \"HDR (gamma)\", \"HDR (log)\"]) üëâ The darker values are now much better represented and we can easily identify the nuances.  Non-linear CRF üéõÔ∏è Up to now, we‚Äôve assumed that the CRF is linear  \\(G(x) = x\\)  . However, we‚Äôve seen that it makes sense for cameras to encode images using a non-linear curve to optimize for dynamic range and appearance.\nüëâ This can result in different pixel values for the same light intensity.  Additional non-linearities might be introduced by other parts of the system ‚Äî e.g. the A/D converter or other digital image processing steps such as white balance correction. The goal of the CRF calibration is to find the overall response of the camera that combines all these factors into a single function.\nCRF Calibration üî¨ In this section, we‚Äôre going to implement a simple but effective algorithm for determining the camera response function from a set of monochrome images. The algorithm is inspired by the approach presented in the paper: A Photometrically Calibrated Benchmark For Monocular Visual Odometry.\nCalibrate from a single pixel üí° Let‚Äôs iteratively discover the approach by first considering an image consisting of 1 pixel.\nGiven our assumptions about the scene, we have full control over the amount of light that a pixel receives by controlling the exposure time. We don‚Äôt know the absolute value but we know that if we double the exposure time, we receive double the amount of light.\nüëâ If we correlate that known light change with the resulting pixel value, we can determine the function that relates them.  The inverse CRF takes a pixel value as an argument and we know that pixel values are integers in the range [0-255]. So we can define it as a discrete function. In Python, this translates to an array where the index of the array is the input to the function.\nU = np.array([0.] * 256) We need to have observations for each possible pixel value to fully determine the inverse CRF. So our calibration dataset needs to start from a fully underexposed image and iteratively increase exposure time until the image is overexposed.\nHere it makes more sense to use arithmetic progression for the exposures so that we have dense coverage over the entire range.\ndef generate_crf_calibration_dataset( E: np.ndarray, G: Callable[[np.ndarray, int], np.ndarray], bit_depth: int = 8, start_ms: int = 1, end_ms: int = 201, step_ms: int = 1, images_per_exposure: int = 1) - ImagesExposuresTuple: \"\"\"Generate dataset with sliding exposure values in the specified range E -- sensor irradiance G -- camera response function bit_depth -- bit depth for the A/D converter of the camera start_ms -- start value (ms) for the range of exposure values end_ms -- end value (ms) for the range of exposure values step_ms -- step size (ms) for the range of exposure values images_per_exposure -- number of images per exposure time \"\"\" exposures = [x / 1e3 for x in range(start_ms, end_ms, step_ms)] exposures *= images_per_exposure images = [] for t in exposures: images.append(capture_image(E=E, t=t, G=G, bit_depth=bit_depth)) return images, exposures Recovering linear response As a first example, we‚Äôre going to generate a dataset using the linear response function and attempt to recover it. We use an arbitrarily chosen pixel irradiance:\nsingle_pixel_irradiance = np.array([1381]) Generate the dataset and verify that it provides a good distribution of pixel values.\nimages, exposures = generate_crf_calibration_dataset( E=single_pixel_irradiance, G=linear_crf) util.display_images(images[0::10], shape=(1,1), annot=False) util.display_observations_distribution(np.array(images)) We don‚Äôt have sufficient images to cover every value from the range (200 images vs 256 values) but we have good enough coverage. Reality isn‚Äôt perfect either so let‚Äôs try to work around that and recover the reverse CRF function U.\n$$U(I(x)) = t\\cdot E(x)$$\ndef recover_U_single_pixel(exposures, images): # Define an arbitrary (relative) sensor irradiance  # Expected to be wrong by a constant factor X sensor_irradiance = np.array([100]) # Initialize the inverse function U to 0s U = np.array([0.] * 256) for t, image in zip(exposures, images): pixel_value = int(image[0]) if pixel_value == 255 or pixel_value == 0: # Skip under- or overexposed pixels continue accumulated_photons = t * sensor_irradiance[0] U[pixel_value] = accumulated_photons return U util.plot_inverse_crf(recover_U_single_pixel(exposures, images)) The zero values are caused by unobserved pixel values. One way we can resolve that is by taking more images with finer exposure steps to cover the entire range of values.\nüëâ We‚Äôre going to interpolate the missing values from their nearest neighbors.  Then we get a very good approximation without stricter requirements on the dataset.\ndef interpolate_missing_values(U): observed_intensity = np.zeros(len(U), dtype=bool) observed_intensity[U != 0.0] = True U[~observed_intensity] = np.interp( (~observed_intensity).nonzero()[0], observed_intensity.nonzero()[0], U[observed_intensity]) return U util.plot_inverse_crf( interpolate_missing_values( recover_U_single_pixel(exposures, images))) The recovered function looks a bit noisy. Debevek \u0026 Malik propose adding a smoothness term based on the sum of squared second derivatives.\nüëâ We‚Äôre going to follow a simpler approach of averaging the values over multiple images from the same exposure.  # Create a dataset with multiple images for each exposure time images, exposures = generate_crf_calibration_dataset( E=single_pixel_irradiance, G=linear_crf, images_per_exposure=20) def recover_U_single_pixel_average(exposures, images): sensor_irradiance = np.array([100]) U_sum = np.array([0.] * 256) U_count = np.array([0] * 256) for t, image in zip(exposures, images): pixel_value = int(image[0]) if pixel_value == 255 or pixel_value == 0: # Skip under- or overexposed pixels continue accumulated_photons = t * sensor_irradiance[0] # Sum all observations for specific pixel value U_sum[pixel_value] += accumulated_photons U_count[pixel_value] += 1 # Calculate the average  observed_intensity = np.zeros(len(U_sum), dtype=bool) observed_intensity[U_count != 0] = True return np.divide(U_sum, U_count, out=np.zeros_like(U_sum), where=observed_intensity) recovered_inverse_crf_linear = interpolate_missing_values( recover_U_single_pixel_average(exposures, images)) util.plot_inverse_crf(recovered_inverse_crf_linear) Nice and smooth üòä I call this a success. Let‚Äôs move onto more interesting examples!\nRecovering gamma response To have meaningful gamma encoding, we need to quantize the images with a 12-bit A/D converter which provides a higher dynamic range that we can then effectively compress down to 8-bit with the gamma encoding. Note that we use 12-bit depth for the camera internal processing but the result is still an 8-bit (gamma encoded) image.\nimages, exposures = generate_crf_calibration_dataset( E=single_pixel_irradiance, G=apply_gamma, images_per_exposure=20, bit_depth=12, start_ms=1, end_ms=3001, step_ms=15) Note that I also had to increase the step size for the exposure sequence to cover the extended dynamic range of the 12-bit A/D converter while keeping the total number of exposures constant between the examples.\nutil.display_images(images[0:200:10], shape=(1,1), annot=False) üí° The gamma dataset (bottom) appears to follow a more linear progression compared to the linear dataset (top). That‚Äôs because your monitor does gamma decoding and wrongly displays non-gamma encoded images.\n  util.display_observations_distribution(np.array(images)) recovered_inverse_crf_gamma = interpolate_missing_values( recover_U_single_pixel_average(exposures, images)) util.plot_inverse_crf(recovered_inverse_crf_gamma) üí° Direct comparison of the relative irradiance values on the Y-axis between the gamma and linear (inverse) response functions reveals the wider range of relative irradiances that the gamma function is capable of encoding (range 0-300 vs 0-18).  Calibrate from multiple pixels üê£ As demonstrated in the previous section, it is possible to calibrate the camera response function by using a single pixel. The downside of this approach is that it requires a lot of images to densely cover the entire range of values and average out the noise ‚Äî we took 20 images @ 200 unique exposures resulting in a total dataset size of 4000 images.\nLet‚Äôs hypothetically consider the other extreme case:\nüëâ What if we have a dataset consisting of a single image with dimensions 64x64?  The image would have 4096 pixels in total. That‚Äôs approximately the same number of pixels as in our dataset with single pixel images. Assuming there‚Äôs a good variation in the pixel irradiances, would this provide us with the same amount of information?\nThe answer is no because we wouldn‚Äôt know the relative irradiances between the pixels.\nüí° In the case of a single pixel, we had a single irradiance value and we could calculate the change in the accumulated light based on the exposure time alone. In order to relate pixel values in the same image, we need to recover the relative sensor irradiance. We already solved this problem in the HDR image section.  The catch is that we already need to know the CRF to recover the irradiance values, but the irradiance values are needed to recover the CRF. The chicken and the egg puzzle?\nIn the following equation, both  \\(U(x)\\)  and  \\(E(x)\\)  are unknown.\n$$U(I(x)) = t\\cdot E(x)$$\nBut we know they‚Äôre both constant so if fix one and solve for the other, we can iteratively converge to a solution. In other words, we‚Äôre going to attempt to evolve the chicken and the egg simultaneously! üê£\ndef recover_U(E: np.ndarray, exposures: List[float], images: List[np.ndarray]): \"\"\"Attempt to recover the inverse CRF (U) Calculation based on the formula U(I(x)) = t * E(x) where I(x), t and E(x) are given as arguments. All observations for a given I(x) are summed up and the average is calculated. Non-observed values are interpolated. E -- sensor irradiance exposures -- list of exposure times t (seconds) images -- images \"\"\" U_sum = np.array([0.] * 256) U_count = np.array([0] * 256) for t, image in zip(exposures, images): for i in range(len(image)): pixel_value = int(image[i]) if pixel_value == 255 or pixel_value == 0: continue # Skip under- or overexposed pixels accumulated_photons = t * E[i] # Sum all observations for specific pixel value U_sum[pixel_value] += accumulated_photons U_count[pixel_value] += 1 # Calculate U as the average of all observations observed_intensity = np.zeros(len(U_sum), dtype=bool) observed_intensity[U_count != 0] = True U = np.divide(U_sum, U_count, out=np.zeros_like(U_sum), where=observed_intensity) return interpolate_missing_values(U) def recover_U_and_E(exposures, images, iterations=5): \"\"\"Attempt to solve for both the inverse CRF and the sensor irradiance Given a dataset, perform a number of iterations by alternatingly solving for U(x) and E(x) to check if they can converge to a solution. \"\"\" E = np.array([100] * len(images[0])) U = np.array([0.] * 256) response_function = lambda x: U[int(x)] U_list = [] E_list = [] for _ in range(iterations): U = recover_U(E, exposures, images) U_list.append(U) E = estimate_sensor_irradiance_weighted( images, exposures, U=np.vectorize(response_function), w=weighting_debevek_malik) E_list.append(E) return U_list, E_list Let‚Äôs generate the dataset:\nimages, exposures = generate_crf_calibration_dataset( E=get_sensor_irradiance(), G=apply_gamma, images_per_exposure=5, bit_depth=12, start_ms=1, end_ms=3001, step_ms=15) I‚Äôve experimented with the parameters until I got good coverage of pixel values for each pixel in the image. Here are the first ten darkest images from the sequence:\nutil.display_images(images[0:10], annot=False) And here‚Äôs a representation of the full (subsampled) dataset:\nutil.display_images(images[0:200:10], annot=False) Now we optimize for both the irradiance image and the inverse response:\nU_list, E_list = recover_U_and_E(exposures, images, iterations = 5); Here are the inverse response functions after each iteration of the optimization:\nutil.plot_multiple_inverse_crf(U_list) And the irradiance images after each iteration:\nutil.display_images([normalize_255(np.log(E + 1)) for E in E_list]) print(\"Normalized Ground-truth: \", normalize_255(get_sensor_irradiance())) print(\"Normalized Estimate: \", normalize_255(E_list[-1])) print(\"Error: \", normalize_255( get_sensor_irradiance()) - normalize_255(E_list[-1]))   Normalized Ground-truth: [ 0. 23. 4. 255.]\nNormalized Estimate: [ 0. 23. 4. 255.]\nError: [0. 0. 0. 0.]\n    This approach quickly converges to the correct solution for both U and E. It starts falling apart if we try it with the more extreme sensor irradiance values.\nimages, exposures = generate_crf_calibration_dataset( E=np.array(get_extreme_sensor_irradiance()), G=apply_gamma, images_per_exposure=5, bit_depth=12, start_ms=1, end_ms=3001, step_ms=15) util.display_images(images[0:10], annot=False) util.display_images(images[0:200:10], annot=False) U_list, E_list = recover_U_and_E(exposures, images, iterations=5); util.plot_multiple_inverse_crf(U_list) util.display_images([normalize_255(np.log(E + 1)) for E in E_list]) Nothing we can‚Äôt resolve by throwing 50 iterations at it üòâ\nU_list, E_list = recover_U_and_E(exposures, images, iterations=50); util.plot_multiple_inverse_crf(U_list[::10]) util.display_images([normalize_255(np.log(E + 1)) for E in E_list[::10]]) print(\"Normalized Ground-truth: \", normalize_255(get_extreme_sensor_irradiance())) print(\"Normalized Estimate: \", normalize_255(E_list[-1])) print(\"Error: \", normalize_255( get_extreme_sensor_irradiance()) - normalize_255(E_list[-1]))   Normalized Ground-truth: [ 0. 5. 15. 255.]\nNormalized Estimate: [ 0. 5. 15. 255.]\nError: [0. 0. 0. 0.]\n    As we‚Äôll see in the next section, real datasets aren‚Äôt quite as extreme and converge within a reasonable number of iterations. In this example, we have only 2x2 images so any extreme values constitute a large percentage of the total pixels. Real-world images might have larger extremes but are constrained to a smaller percentage of the image.\nCalibrate from a real dataset üéâ And finally, we get to test our algorithms on a non-simulated dataset acquired with an actual camera! Yay!\nYou can download the dataset that I‚Äôve used from here: Monocular Visual Odometry Dataset. We‚Äôre going to be using only two sequences from the dataset: narrow_sweep3 and narrowGamma_sweep3.\nIf you want to create a calibration dataset for your camera, you need to be able to programmatically control the exposure time. The camera needs to be static and point towards a static scene with good variability in surface reflectances. Ideally, you want to have close to a flat histogram. The light source has to be constant ‚Äî beware that LED lights usually are not! You can use sunlight on a non-cloudy day.\nOnce you have that, you can fix the gain of your sensor to the lowest value to minimize noise and swipe the exposure time ( \\(t\\)  ) in small  \\(\\Delta t\\)  steps from the minimum value until the image becomes fully or mostly overexposed ‚Äî the same way we‚Äôve done in our simulated datasets.\nimport os from skimage import io from skimage.measure import block_reduce def load_dataset(path: str, with_downsample: bool = True, block_size: Tuple[int, int] = (10, 10)): \"\"\"Load dataset from the specified path Subsambling the image size in order reduce the execution time of the (unoptimized) example algorithms path -- path to the dataset with_downsample -- enable/disable downsampling block_size -- block size that gets sampled down to 1 pixel \"\"\" image_folder = os.path.join(path, \"images\") exposure_file = os.path.join(path, \"times.txt\") gt_file = os.path.join(path, \"pcalib.txt\") exposures, images = [], [] shape_original, shape_output = None, None def downsample(image): \"\"\"Do max sampling in order to avoid bias from overexposed values\"\"\" return block_reduce(image, block_size=block_size, func=np.max, cval=0) with open(exposure_file, \"r\") as f: for line in f.readlines(): # Parse meta information and append to output lists [idx, ts, exposure_ms] = line.strip().split(\" \") exposures.append(float(exposure_ms) * 1e-3) # Load and downsamble image image_original = io.imread(os.path.join( image_folder, idx + \".jpg\"), as_gray=True) image_output = downsample(image_original) \\ if with_downsample else image_original images.append(image_output.flatten()) if shape_output is None: shape_original = image_original.shape shape_output = image_output.shape print(\"Original size: \" + str(shape_original) + \" Output size: \" + str(shape_output)) ground_truth = None with open(gt_file, 'r') as f: ground_truth = [float(i) for i in f.readline().strip().split()] return exposures, images, shape_output, ground_truth Let‚Äôs load the (downsampled) linear response dataset:\nexposures, images, shape, ground_truth = load_dataset( path=\"data/calib_narrow_sweep3\")   Original size: (1024, 1280) Output size: (103, 128)\n    util.display_images(images[::200], shape=shape, annot=False) I‚Äôm going to use the calibration results provided with the Monocular Visual Odometry Dataset as a reference ground-truth.\nLet‚Äôs attempt to calibrate:\nU_list, E_list = recover_U_and_E(exposures, images, iterations=3); util.plot_multiple_inverse_crf(U_list) util.display_images([normalize_255(np.log(E + 1)) for E in E_list], shape=shape, annot=False) üëâ Great, in just 3 iterations our algorithm converges to the correct solution!  Let‚Äôs try it on the gamma encoded dataset as well.\nexposures, images, shape, ground_truth = load_dataset( path=\"data/calib_narrowGamma_sweep3\")   Original size: (1024, 1280) Output size: (103, 128)\n    util.display_images(images[::200], shape=shape, annot=False) U_list, E_list = recover_U_and_E(exposures, images, iterations=3); util.plot_multiple_inverse_crf(U_list) util.display_images([normalize_255(np.log(E + 1)) for E in E_list], shape=shape, annot=False) It is amazing to see how well it works even with 1/10 of the available information after subsampling the images from their original size of (1024, 1280) to (103, 128).\nüí° Since we haven‚Äôt enforced any constraints on the resulting (inverse) response function, we can calibrate for any non-linearity.  üí° An analogous approach can be used to calibrate RGB images. The process can be repeated for each of the R, G, B channels separately.  Final Words üèÅ Although simple in its concept, the camera response function lies at the intersection of sensor physics and digital image processing. This makes it a fascinating topic to explore in-depth as it can lead our curiosity to a variety of unexplored paths.\nWe‚Äôve barely scratched the surface of camera sensor physics, simulation, noise models and applications of the CRF. This article should hopefully provide a convenient starting point for self-learning and further research.\nüí° Have a robotics topic in mind that is inadequately covered by existing learning resources? Feel free to reach out to me on üê¶ twitter and it might be next. ;-)   ",
  "wordCount" : "7262",
  "inLanguage": "en",
  "image":"https://roboalgorithms.com/crf/crf-cover.png","datePublished": "2021-02-27T17:00:00+01:00",
  "dateModified": "2021-02-27T17:00:00+01:00",
  "author":{
    "@type": "Person",
    "name": "Martin Mihaylov"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://roboalgorithms.com/posts/camera-response-function/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "RoboAlgorithms",
    "logo": {
      "@type": "ImageObject",
      "url": "https://roboalgorithms.com/favicon.ico"
    }
  }
}
</script>



</head>

<body class="" id="top">
<noscript>
    <style type="text/css">
        .theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://roboalgorithms.com" accesskey="h" title="RoboAlgorithms (Alt + H)">RoboAlgorithms</a>
            <span class="logo-switches">
                
            </span>
        </div>
        <ul class="menu" id="menu" onscroll="menu_on_scroll()"></ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">
      Camera Response Function
    </h1>
    <div class="post-description">
      Explore the fundamentals of the camera response function with Python.
    </div>
    <div class="post-meta">

February 27, 2021&nbsp;¬∑&nbsp;Martin Mihaylov

    </div>
  </header> 
<figure class="entry-cover"><img src="https://roboalgorithms.com/crf/crf-cover.png" alt="">
        
</figure>

  <div class="toc">
    <details >
      <summary accesskey="c" title="(Alt + C)">
        <div class="details">Table of Contents</div>
      </summary>
      <div class="inner"><ul><li>
        <a href="#first-principles-exploration-" aria-label="First principles exploration üëã">First principles exploration üëã</a><ul>
            <li>
        <a href="#what-the-crf-" aria-label="What the CRF? ü§î">What the CRF? ü§î</a></li><li>
        <a href="#applications-of-the-crf-" aria-label="Applications of the CRF üî®">Applications of the CRF üî®</a></li></ul>
    </li><li>
        <a href="#how-cameras-measure-light-" aria-label="How cameras measure light ‚òÄÔ∏è">How cameras measure light ‚òÄÔ∏è</a><ul>
            <li>
        <a href="#converting-voltages-to-pixel-values--" aria-label="Converting voltages to pixel values  üì∫">Converting voltages to pixel values  üì∫</a><ul>
            <li>
        <a href="#analog-to-digital-converter-" aria-label="Analog-to-Digital Converter üìà">Analog-to-Digital Converter üìà</a><ul>
            <li>
        <a href="#quantization-noise" aria-label="Quantization noise">Quantization noise</a></li><li>
        <a href="#under--and-overexposure" aria-label="Under- and overexposure">Under- and overexposure</a></li><li>
        <a href="#dynamic-range" aria-label="Dynamic Range">Dynamic Range</a></li></ul>
    </li><li>
        <a href="#programmable-gain-amplifier-" aria-label="Programmable Gain Amplifier ‚¨õ">Programmable Gain Amplifier ‚¨õ</a></li><li>
        <a href="#image-acquisition-pipeline-" aria-label="Image Acquisition Pipeline üì∏">Image Acquisition Pipeline üì∏</a></li></ul>
    </li><li>
        <a href="#further-recommended-reading-" aria-label="Further Recommended Reading üìö">Further Recommended Reading üìö</a></li></ul>
    </li><li>
        <a href="#simulating-a-simple-camera-" aria-label="Simulating a simple camera üì∑">Simulating a simple camera üì∑</a><ul>
            <li>
        <a href="#disclaimer-%e2%84%b9" aria-label="Disclaimer ‚ÑπÔ∏è">Disclaimer ‚ÑπÔ∏è</a></li><li>
        <a href="#assumptions-" aria-label="Assumptions üìù">Assumptions üìù</a><ul>
            <li>
        <a href="#camera" aria-label="Camera">Camera</a></li><li>
        <a href="#scene" aria-label="Scene">Scene</a></li></ul>
    </li><li>
        <a href="#simulating-a-2x2-pixel-sensor-" aria-label="Simulating a 2x2 pixel sensor üß±">Simulating a 2x2 pixel sensor üß±</a></li></ul>
    </li><li>
        <a href="#hdr-images-" aria-label="HDR Images ü™Ç">HDR Images ü™Ç</a><ul>
            <li>
        <a href="#inverse-crf-" aria-label="Inverse CRF üîô">Inverse CRF üîô</a></li><li>
        <a href="#recover-image-irradiance-" aria-label="Recover Image Irradiance üñºÔ∏è">Recover Image Irradiance üñºÔ∏è</a></li><li>
        <a href="#recover-sensor-irradiance-" aria-label="Recover Sensor Irradiance üì∑">Recover Sensor Irradiance üì∑</a><ul>
            <li>
        <a href="#improving-the-sensor-irradiance-estimate" aria-label="Improving the sensor irradiance estimate">Improving the sensor irradiance estimate</a></li></ul>
    </li><li>
        <a href="#absolute-vs-relative-irradiance-" aria-label="Absolute vs Relative Irradiance ‚ö†Ô∏è">Absolute vs Relative Irradiance ‚ö†Ô∏è</a></li><li>
        <a href="#displaying-hdr-images-" aria-label="Displaying HDR Images üñ•Ô∏è">Displaying HDR Images üñ•Ô∏è</a></li><li>
        <a href="#gamma-encoding-" aria-label="Gamma encoding üìà">Gamma encoding üìà</a></li></ul>
    </li><li>
        <a href="#non-linear-crf-" aria-label="Non-linear CRF üéõÔ∏è">Non-linear CRF üéõÔ∏è</a></li><li>
        <a href="#crf-calibration-" aria-label="CRF Calibration üî¨">CRF Calibration üî¨</a><ul>
            <li>
        <a href="#calibrate-from-a-single-pixel-" aria-label="Calibrate from a single pixel üí°">Calibrate from a single pixel üí°</a><ul>
            <li>
        <a href="#recovering-linear-response" aria-label="Recovering linear response">Recovering linear response</a></li><li>
        <a href="#recovering-gamma-response" aria-label="Recovering gamma response">Recovering gamma response</a></li></ul>
    </li><li>
        <a href="#calibrate-from-multiple-pixels-" aria-label="Calibrate from multiple pixels üê£">Calibrate from multiple pixels üê£</a></li><li>
        <a href="#calibrate-from-a-real-dataset-" aria-label="Calibrate from a real dataset üéâ">Calibrate from a real dataset üéâ</a></li></ul>
    </li><li>
        <a href="#final-words-" aria-label="Final Words üèÅ">Final Words üèÅ</a></li></ul>
      </div>
    </details>
  </div>
  <div class="post-content">
<h1 id="first-principles-exploration-">First principles exploration üëã<a hidden class="anchor" aria-hidden="true" href="#first-principles-exploration-">#</a></h1>
<p>The goal of this article is to explore the topic of the camera response function experimentally. We&rsquo;ll build
up from first principles and the main focus will be on building intuition with code examples. The only pre-requisite is base
familiarity with Python + NumPy and, of course, curiosity about the topic. üßë‚ÄçüöÄ</p>
<p>If you&rsquo;re looking for a quick way to find out the response function of your camera, you can check out the
implementation of <a href="https://docs.opencv.org/3.4/da/d27/classcv_1_1CalibrateDebevec.html">CalibrateDebevec</a> in OpenCV, or stick around to get a deeper intuition.</p>
<p>I&rsquo;ve made an effort to link to interesting resources that provide a good starting point to dig deeper into specific directions
that could not be covered in this article. You&rsquo;re encouraged to follow them and come back to this at your own pace.</p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">You can follow along and play with the code examples. The Jupyter notebook is <a href="https://github.com/martomi/roboalgorithms-code/blob/main/notebooks/camera-response-function.ipynb">available on
Github</a>. üìî</div>
</div>

<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">Subscribe to the <a href="https://www.youtube.com/channel/UCB_nIiGE8XgF2166Sl1ewUg">RoboAlgorithms YT
channel</a> to be notified about the complementary video on
the camera response function (coming out soon). üé•</div>
</div>

<h2 id="what-the-crf-">What the CRF? ü§î<a hidden class="anchor" aria-hidden="true" href="#what-the-crf-">#</a></h2>
<p>A camera measures the light intensity hitting the imaging sensor and outputs an image with pixel values that in some way
correspond to that light intensity. This correspondence is modeled by the camera response function (CRF).</p>
<p><img src="/crf/crf-animation.gif#center" alt="Camera Response Function Animation"></p>
<p>For a computer vision engineer, the pixel values are the only observation of the real-world. Thus, it is helpful to understand
how they relate to actual light intensities to correctly model that world.</p>
<p>Most cameras have some non-linearity in their response to light intensities. Images taken by consumer-grade cameras are particularly
affected by this because they are optimized for viewing experience and not for measurement accuracy.</p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">If we know the response function, we can resolve that ambiguity and turn any camera
into a scientific measurement device! üî¨</div>
</div>

<h2 id="applications-of-the-crf-">Applications of the CRF üî®<a hidden class="anchor" aria-hidden="true" href="#applications-of-the-crf-">#</a></h2>
<p>Many computer vision algorithms that rely on the <a href="http://www.cs.cmu.edu/~16385/s17/Slides/14.1_Brightness_Constancy.pdf">brightness constancy
assumption</a> require accurate estimation of the
CRF. For example, <a href="https://en.wikipedia.org/wiki/Optical_flow">optical flow</a> or any direct method for visual SLAM that
works with pixel intensities such as <a href="https://vision.in.tum.de/research/vslam/dso">DSO</a>.</p>
<p>Direct methods for visual SLAM are currently one of the most robust approaches for accurate robot localization in
challenging environments where GNSS coverage is unreliable. An autonomous last-mile robot delivery service is an example that&rsquo;s
increasingly gaining <a href="https://www.wired.com/story/amazon-fedex-delivery-robots-your-sidewalk/">commercial traction</a> in recent years
and may use such technology.</p>
<p><img src="/crf/delivery-robot.png#center" alt="Delivery Robot"></p>
<div class="alert alert-info">
  <div class="alert-icon">üí°</div>
  <div class="alert-content">One could argue that due to the shift towards deep learning approaches and more readily available cameras that are optimized
for machine vision, it will be unnecessary to manually calibrate for the camera response function in the future. Nevertheless, I still
see value in exploring the fundamentals for learning purposes.</div>
</div>

<p>In this article, we&rsquo;ll look at the application of generating <a href="https://people.eecs.berkeley.edu/~malik/papers/debevec-malik97.pdf">HDR images</a> which
also relies on knowledge about the camera response function because we need to &ldquo;fuse&rdquo; multiple images taken at different exposure times.</p>
<h1 id="how-cameras-measure-light-">How cameras measure light ‚òÄÔ∏è<a hidden class="anchor" aria-hidden="true" href="#how-cameras-measure-light-">#</a></h1>
<p>A camera is an array of <a href="https://en.wikipedia.org/wiki/Photodiode">photodiodes</a> that converts incoming
<a href="https://en.wikipedia.org/wiki/Photon">photons</a> into electrical current or
<a href="https://en.wikipedia.org/wiki/Voltage">voltage</a>. This phenomenon is known as the
<a href="https://en.wikipedia.org/wiki/Photoelectric_effect">photoelectric effect</a> and is the
same phenomenon utilized in solar panels.</p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">We measure the voltage to determine the amount of incident light.</div>
</div>

<p>For each <a href="https://en.wikipedia.org/wiki/Active-pixel_sensor#Pixel">pixel</a>, there&rsquo;s a corresponding photodiode in the
camera sensor. We take an image by exposing the sensor to light for a given period (exposure time). Since the
photodiodes produce an electrical current proportional to the amount of light they receive, we can use that measurement to
reconstruct the image.</p>
<p><img src="/crf/photodiode-to-image.png#center" alt="Photodiode to Image"></p>
<h2 id="converting-voltages-to-pixel-values--">Converting voltages to pixel values  üì∫<a hidden class="anchor" aria-hidden="true" href="#converting-voltages-to-pixel-values--">#</a></h2>
<p>When we measure the voltage generated by the photodiode we get an <a href="https://en.wikipedia.org/wiki/Analog_signal">analog</a>
quantity. But we need a <a href="https://en.wikipedia.org/wiki/Digital_signal">digital</a> value that fits into our pixel
representation of 8 bits. A device that can perform this conversion is called an <a href="https://en.wikipedia.org/wiki/Analog-to-digital_converter">Analog-Digital (A/D)
converter</a>. Most A/D converters used in cameras today will output
an 8, 12, or 16-bit digital value.</p>
<h3 id="analog-to-digital-converter-">Analog-to-Digital Converter üìà<a hidden class="anchor" aria-hidden="true" href="#analog-to-digital-converter-">#</a></h3>
<p>We will start with an 8-bit A/D converter. Note that regardless of the bit depth, the range of possible values will be limited in
the digital domain. Meaning that with 8-bit we only have <span>
  \(2^{8} = 256\)
</span>
, unique values to assign to all possible voltage
measurements.</p>
<p>Let&rsquo;s simulate a naive A/D converter:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">max_value</span>(bit_depth: int) <span style="color:#f92672">-&gt;</span> int:
    <span style="color:#e6db74">&#34;&#34;&#34;Return max integer value for the specified bit depth&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">2</span><span style="color:#f92672">**</span>bit_depth <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">adc</span>(milivolts: np<span style="color:#f92672">.</span>ndarray, bit_depth: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">8</span>) <span style="color:#f92672">-&gt;</span> int:
    <span style="color:#e6db74">&#34;&#34;&#34;Naive A/D converter
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    milivolts  -- accumulated voltage from a photodiode
</span><span style="color:#e6db74">    bit_depth  -- target bit depth of the digitized output
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>clip(np<span style="color:#f92672">.</span>around(milivolts), <span style="color:#ae81ff">0</span>, max_value(bit_depth))

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_adc_examples</span>():
    <span style="color:#e6db74">&#34;&#34;&#34;Plot a few interesting examples&#34;&#34;&#34;</span>
    f, axes <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">3</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">20</span>, <span style="color:#ae81ff">5</span>))
    x_label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Input voltage (mV)&#39;</span>
    y_label <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Output pixel value&#39;</span>

    voltages <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">0.1</span>)
    util<span style="color:#f92672">.</span>plot(axes[<span style="color:#ae81ff">0</span>], voltages, adc(voltages), 
              <span style="color:#e6db74">&#34;In supported range&#34;</span>, x_label, y_label)

    voltages <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.01</span>)
    util<span style="color:#f92672">.</span>plot(axes[<span style="color:#ae81ff">1</span>], voltages, adc(voltages), 
              <span style="color:#e6db74">&#34;Below supported range&#34;</span>, x_label, y_label)

    voltages <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">255</span>, <span style="color:#ae81ff">500</span>, <span style="color:#ae81ff">1</span>)
    util<span style="color:#f92672">.</span>plot(axes[<span style="color:#ae81ff">2</span>], voltages, adc(voltages), 
              <span style="color:#e6db74">&#34;Above supported range&#34;</span>, x_label, y_label)
    
plot_adc_examples()
</code></pre></div><p><img src="/crf/adc-example.png" alt="A/D Converter Output"></p>
<h4 id="quantization-noise">Quantization noise<a hidden class="anchor" aria-hidden="true" href="#quantization-noise">#</a></h4>
<p>From the staircase-like plot above we can see the discrete jumps caused by the <a href="https://en.wikipedia.org/wiki/Quantization_(signal_processing)">quantization</a> effects. Because we&rsquo;re losing some precision at this step, this effect also becomes a component of the <a href="https://en.wikipedia.org/wiki/Image_noise">total image noise</a> known as the <a href="https://en.wikipedia.org/wiki/Image_noise#Quantization_noise_(uniform_noise)">quantization noise</a>.</p>
<h4 id="under--and-overexposure">Under- and overexposure<a hidden class="anchor" aria-hidden="true" href="#under--and-overexposure">#</a></h4>
<p>The plot also demonstrates that any values below or above the supported range are irreversibly lost. This phenomenon is known as under- or overexposure.
Our goal will always be to choose the exposure time such that this effect is minimized.</p>
<h4 id="dynamic-range">Dynamic Range<a hidden class="anchor" aria-hidden="true" href="#dynamic-range">#</a></h4>
<p>The range of light intensities that the camera can capture without under- or overexposure is called the camera dynamic range. It is a very important
metric for outdoor robot operation due to the large variations in light intensities.</p>
<div class="alert alert-info">
  <div class="alert-icon">üí°</div>
  <div class="alert-content">Curiously, extending the dynamic range of the camera isn&rsquo;t as simple as increasing the bit-depth of the A/D converter. The primary limiting factor is usually linked to the physical <a href="https://hamamatsu.magnet.fsu.edu/articles/dynamicrange.html">full-well capacity and noise level</a> of the photodiodes. The bit-depth of the A/D converter is then selected to maximize that.</div>
</div>

<p>So our goal is to maximize the information we can get from every image. We have two ways to optimize for that:</p>
<ol>
<li>Choose a camera with a higher dynamic range</li>
<li>Adjust the exposure time to minimize over- and underexposure</li>
</ol>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">For very dimly lit scenarios, it is not always an option to adjust the exposure time because a very long exposure time can cause <a href="https://en.wikipedia.org/wiki/Motion_blur">motion blur</a>.</div>
</div>

<h3 id="programmable-gain-amplifier-">Programmable Gain Amplifier ‚¨õ<a hidden class="anchor" aria-hidden="true" href="#programmable-gain-amplifier-">#</a></h3>
<p>Underexposure happens for dimly lit scenes or shorter exposure times when the photodiodes don&rsquo;t have a chance to convert a sufficient amount of photons into electrical current. This is how a fully underexposed image looks:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">photodiode_array_milivolts <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.028</span>, <span style="color:#ae81ff">0.213</span>, <span style="color:#ae81ff">0.234</span>, <span style="color:#ae81ff">0.459</span>])

pixel_values <span style="color:#f92672">=</span> adc(photodiode_array_milivolts)
util<span style="color:#f92672">.</span>display_images([pixel_values], titles<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;Underexposed image&#34;</span>])
</code></pre></div><p><img src="/crf/underexposed-image.png#center" alt="Underexposed Image"></p>
<p>Cameras have a <a href="https://en.wikipedia.org/wiki/Programmable-gain_amplifier">programmable-gain amplifier</a> (PGA) that can amplify the voltages from the photodiodes and bring them in the range supported by the A/D converter. The level of amplification can be configured on most cameras through the ISO setting.</p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">We can capture images in the dark without increasing the exposure time as much.</div>
</div>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">pga</span>(milivolts: np<span style="color:#f92672">.</span>ndarray, gain: float <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>) <span style="color:#f92672">-&gt;</span> float:
    <span style="color:#e6db74">&#34;&#34;&#34;Naive programmable-gain amplifier
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    milivolts  -- input voltage from photodiode
</span><span style="color:#e6db74">    gain       -- ratio output / input
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">return</span> gain <span style="color:#f92672">*</span> milivolts

<span style="color:#75715e"># Add an amplifier to the same example values from above</span>
pixel_values <span style="color:#f92672">=</span> adc(pga(photodiode_array_milivolts, gain<span style="color:#f92672">=</span><span style="color:#ae81ff">300.0</span>))
util<span style="color:#f92672">.</span>display_images([pixel_values], titles<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;Amplified image&#34;</span>])
</code></pre></div><p><img src="/crf/amplified-image.png#center" alt="Amplified Image"></p>
<p>There&rsquo;s a caveat: we did not model <a href="https://en.wikipedia.org/wiki/Image_noise">image noise</a>. The weaker signal in dimply lit scenes results in a lower <a href="https://en.wikipedia.org/wiki/Signal-to-noise_ratio">signal-to-noise ratio (SNR)</a>. In other words, amplifying the signal will also amplify the noise and that&rsquo;s why using a higher gain usually results in grainier and lower quality images. In extreme cases when the strength of the signal is below the <a href="https://en.wikipedia.org/wiki/Noise_floor">noise floor</a> it can be difficult to extract <em>any</em> useful information.</p>
<p>Reducing the image noise can be performed in the <a href="https://en.wikipedia.org/wiki/Digital_image_processing">digital image processing</a> pipeline which is the final step before saving the image. This part significantly differs between camera models and may include <a href="https://en.wikipedia.org/wiki/Color_balance">white balance</a>, <a href="https://en.wikipedia.org/wiki/Tone_mapping">tone mapping</a>, <a href="https://en.wikipedia.org/wiki/Gamma_correction">gamma encoding</a>, <a href="https://en.wikipedia.org/wiki/Image_compression">image compression</a> and <a href="https://en.wikipedia.org/wiki/Color_image_pipeline">more</a>.</p>
<h3 id="image-acquisition-pipeline-">Image Acquisition Pipeline üì∏<a hidden class="anchor" aria-hidden="true" href="#image-acquisition-pipeline-">#</a></h3>
<p>The figure below provides an overview of the discussed components.</p>
<p><img src="/crf/camera-simplified.png#center" alt="Overview Camera Components"></p>
<div class="alert alert-info">
  <div class="alert-icon">üí°</div>
  <div class="alert-content">The camera response function is a single function that approximates the response of the whole system with all components inside. More on this later.</div>
</div>

<h2 id="further-recommended-reading-">Further Recommended Reading üìö<a hidden class="anchor" aria-hidden="true" href="#further-recommended-reading-">#</a></h2>
<p>I&rsquo;m aware that I&rsquo;ve touched upon a lot of topics very quickly. If you don&rsquo;t have a good intuition already, I highly recommend reading
this amazing interactive introduction on <a href="https://ciechanow.ski/cameras-and-lenses/">Cameras and Lenses</a>. Seriously, please go check this article out.</p>
<p><a href="https://youtu.be/MytCfECfqWc">The Science of Camera Sensors</a> is another great resource in video form that provides an excellent introduction to the physics of how photodiodes and camera sensors work.</p>
<p><strong>Advanced</strong>: For a more in-depth look into the theory and physics behind CMOS photodiodes, chapter 3 from <a href="https://tel.archives-ouvertes.fr/tel-01062484/document">this dissertation</a> provides a good overview.</p>
<h1 id="simulating-a-simple-camera-">Simulating a simple camera üì∑<a hidden class="anchor" aria-hidden="true" href="#simulating-a-simple-camera-">#</a></h1>
<h2 id="disclaimer-‚Ñπ">Disclaimer ‚ÑπÔ∏è<a hidden class="anchor" aria-hidden="true" href="#disclaimer-‚Ñπ">#</a></h2>
<p>For the sake of brevity, many of the physical aspects of cameras are oversimplified in the presented examples. They are only meant to aid the intuition and shouldn&rsquo;t be relied upon for accuracy or completeness. However, if you spot any mistakes, I would love to hear about them at üì® <script type="text/javascript"
       nonce="dXNlcj0iaGVsbG8iLGRvbWFpbj0iaGVua3ZlcmxpbmRlLmNvbSIsZG9jdW1lbnQud3JpdGUodXNlcisiQCIrZG9tYWluKTs=">
  user="feedback",domain="roboalgorithms.com",document.write(user+"@"+domain);
</script>
<noscript>feedback at roboalgorithms.com</noscript>
 or on <a href="https://twitter.com/martinmihaylov_">üê¶ twitter</a>.</p>
<h2 id="assumptions-">Assumptions üìù<a hidden class="anchor" aria-hidden="true" href="#assumptions-">#</a></h2>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">The following assumptions provide the framework for the rest of the article. Feel free to skip this section and refer back to it later if you want to jump to the fun part already. Some parts may only become clear after completing the entire article.</div>
</div>

<h3 id="camera">Camera<a hidden class="anchor" aria-hidden="true" href="#camera">#</a></h3>
<ul>
<li>uses the <a href="https://en.wikipedia.org/wiki/Pinhole_camera_model">pinhole camera model</a> ‚Äî i.e. no modeling of camera optics</li>
<li>is <a href="https://en.wikipedia.org/wiki/Monochrome">monochrome</a> ‚Äî i.e. no simulation of the <a href="https://en.wikipedia.org/wiki/Color_filter_array">color filter array</a> and <a href="https://en.wikipedia.org/wiki/Demosaicing">demosaicing</a></li>
<li><a href="https://en.wikipedia.org/wiki/List_of_monochrome_and_RGB_color_formats#8-bit_Grayscale">8-bit grayscale</a> output format is used for the images</li>
<li>static camera ‚Äî there&rsquo;s no motion during or between image acquisitions</li>
</ul>
<div class="alert alert-info">
  <div class="alert-icon">üí°</div>
  <div class="alert-content">Despite these simplifying assumptions, the concepts learned here are applicable to real-world RGB cameras with any kind of optical lens. Try to understand why.</div>
</div>

<h3 id="scene">Scene<a hidden class="anchor" aria-hidden="true" href="#scene">#</a></h3>
<ul>
<li>receives constant light intensity from the light source (brightness constancy)</li>
<li>static ‚Äî no moving objects in the scene during or between image acquisitions</li>
</ul>
<p>In a practical sense, it means that we cannot accurately determine the camera response function on a partly cloudy day while relying on sunlight as our light source because the clouds may result in significant changes in the <a href="https://en.wikipedia.org/wiki/Radiant_flux">radiant flux</a> received by the scene.</p>
<div class="alert alert-info">
  <div class="alert-icon">üí°</div>
  <div class="alert-content">Beware that some artificial light sources might also <a href="https://en.wikipedia.org/wiki/Pulse-width_modulation">flicker</a>.</div>
</div>

<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">Given the above assumptions, if we take multiple images with the same exposure time <strong>t</strong> we must always get the same pixel values (not accounting for image noise).</div>
</div>

<h2 id="simulating-a-2x2-pixel-sensor-">Simulating a 2x2 pixel sensor üß±<a hidden class="anchor" aria-hidden="true" href="#simulating-a-2x2-pixel-sensor-">#</a></h2>
<p>We will use the term pixel <a href="https://en.wikipedia.org/wiki/Irradiance">irradiance</a> to refer to the number of photons hitting the physical area of a pixel per second (<span>
  \(m^{‚àí2}‚ãÖs^{‚àí1}\)
</span>
). You can also refer to SI photon units definition on the wiki page of <a href="https://en.wikipedia.org/wiki/Photon_counting">photon counting</a>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_sensor_irradiance</span>():
    <span style="color:#e6db74">&#34;&#34;&#34;Get a sample sensor irradiance
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    We have an imaging sensor of 2x2 pixels (stored as 1D array)
</span><span style="color:#e6db74">    The sensor irradiance encodes how many photons
</span><span style="color:#e6db74">    are hitting the image sensor every second.
</span><span style="color:#e6db74">    The values are arbitrarily chosen.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    pix11_irradiance <span style="color:#f92672">=</span> <span style="color:#ae81ff">1122</span> <span style="color:#75715e"># photons / second</span>
    pix12_irradiance <span style="color:#f92672">=</span> <span style="color:#ae81ff">3242</span> <span style="color:#75715e"># photons / second</span>
    pix21_irradiance <span style="color:#f92672">=</span> <span style="color:#ae81ff">1452</span> <span style="color:#75715e"># photons / second</span>
    pix22_irradiance <span style="color:#f92672">=</span> <span style="color:#ae81ff">25031</span> <span style="color:#75715e"># photons / second</span>

    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>array([
        pix11_irradiance, pix12_irradiance,
        pix21_irradiance, pix22_irradiance])
</code></pre></div><p><img src="/crf/crf-animation.gif#center" alt="Camera Response Function Animation"></p>
<p>Let&rsquo;s start with the simple case of a <em>linear</em> camera response function. This intuitively means that doubling the amount of light (pixel irradiance) would result in doubling the observed pixel value. We define the CRF as:</p>
<p>$$CRF = G(x) = a\cdot x + b$$</p>
<p>We&rsquo;re going to set <span>
  \(a = 1\)
</span>
 and <span>
  \(b = 0\)
</span>
 and assume that each photon increases the voltage by 1mV. This is easy to simulate but unfortunately, 100% detached from reality. Accurate simulation of that process is not in the scope of the article but feel free to enter that rabbit hole starting from <a href="https://en.wikipedia.org/wiki/Quantum_efficiency">quantum efficiency</a> ;-). We also assume white Gaussian noise which approximates <a href="https://en.wikipedia.org/wiki/Johnson%E2%80%93Nyquist_noise">thermal noise</a>. All <a href="https://www.researchgate.net/publication/228612445_A_model_for_measurement_of_noise_in_CCD_digital-video_cameras">other sources</a> of noise will be ignored.</p>
<p>$$CRF = G(x) = x$$</p>
<p>To get an estimate of the voltage accumulated in each pixel, we need to integrate the irradiance over the period of the exposure time <strong>t</strong>. We define taking an image as:</p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">$$I(k) = G(adc(pga(t\cdot E(k))))$$</div>
</div>

<p>where <strong>E</strong> is the sensor irradiance (i.e. an array of all pixel irradiances), <strong>G</strong> is the camera response function as already defined above, <strong>t</strong> is the exposure time in seconds, <strong>I</strong> is the image intensity or observed pixel value and <strong>k</strong> is the index of the pixel.</p>
<div class="alert alert-info">
  <div class="alert-icon">üí°</div>
  <div class="alert-content"><p>In the literature, you may find the equation defined in its simpler form:</p>
<p>$$I(k) = G^{*}(t\cdot E(k))$$</p>
<p>where the camera response function is:</p>
<p>$$G^{*}(x) = G(adc(pga(x))$$</p>
<p>I&rsquo;ve chosen to keep the functions separate here so that the logic for amplification and quantization is not intermingled with the CRF in the Python code.</p>
</div>
</div>

<p>Let&rsquo;s code this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> typing <span style="color:#f92672">import</span> Callable

ResponseFunction <span style="color:#f92672">=</span> Callable[[np<span style="color:#f92672">.</span>ndarray, int], np<span style="color:#f92672">.</span>ndarray]

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">capture_image</span>(E: np<span style="color:#f92672">.</span>ndarray, t: float, G: ResponseFunction, 
                 bit_depth: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">8</span>, std_noise: float <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span>) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
    <span style="color:#e6db74">&#34;&#34;&#34;Simulate capturing an image with exposure time t.
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    You can specify higher bit_depth for internal processing prior to
</span><span style="color:#e6db74">    finally converting the image to 8-bit depth before output.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    E         -- sensor irradiance
</span><span style="color:#e6db74">    t         -- exposure time (seconds)
</span><span style="color:#e6db74">    G         -- camera response function (CRF)
</span><span style="color:#e6db74">    bit_depth -- bit depth for A/D converter output and response function input
</span><span style="color:#e6db74">    std_noise -- standard deviation for the Gaussian noise distribution
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#75715e"># Non-realistic but simple calculation</span>
    accumulated_milivolts <span style="color:#f92672">=</span> E <span style="color:#f92672">*</span> t
    
    <span style="color:#75715e"># Add the white Gaussian noise</span>
    noise <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0</span>, std_noise, size<span style="color:#f92672">=</span>len(accumulated_milivolts))
    accumulated_milivolts <span style="color:#f92672">+=</span> noise
    
    image <span style="color:#f92672">=</span> G(
        adc(pga(accumulated_milivolts), bit_depth), bit_depth
    )
    
    <span style="color:#75715e"># Always convert to 8-bit depth prior to saving / outputting</span>
    to_8bit_depth <span style="color:#f92672">=</span> max_value(<span style="color:#ae81ff">8</span>) <span style="color:#f92672">/</span> max_value(bit_depth)
    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>around(image <span style="color:#f92672">*</span> to_8bit_depth)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">linear_crf</span>(X: np<span style="color:#f92672">.</span>ndarray, bit_depth: int) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
    <span style="color:#e6db74">&#34;&#34;&#34;Linear camera response function G(x) = x&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">return</span> X

</code></pre></div><p>Now take a couple of images with different exposures:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">img1 <span style="color:#f92672">=</span> capture_image(E<span style="color:#f92672">=</span>get_sensor_irradiance(), t<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">10</span>, G<span style="color:#f92672">=</span>linear_crf)
img2 <span style="color:#f92672">=</span> capture_image(E<span style="color:#f92672">=</span>get_sensor_irradiance(), t<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">100</span>, G<span style="color:#f92672">=</span>linear_crf)
util<span style="color:#f92672">.</span>display_images([img1, img2], 
                    titles<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;Exposure: t = 1/10 second&#34;</span>, 
                            <span style="color:#e6db74">&#34;Exposure: t = 1/100 second&#34;</span>])
</code></pre></div><p><img src="/crf/first-two-pictures.png#center" alt="Two Pictures Example"></p>
<p>We can correctly expose all pixels with 1/100 second exposure time (none of the pixels are 0 or 255).
It becomes more interesting if we have a scene with more extreme values which cannot be captured by the dynamic range of the camera sensor.</p>
<p>Think of an image from inside a cave looking out.</p>
<p><img src="/crf/hdr.jpg#center" alt="Cave Looking Out"></p>
<!-- raw HTML omitted -->
<p>We can simulate this by choosing more extreme values for the sensor irradiance:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_extreme_sensor_irradiance</span>():
    <span style="color:#75715e"># Note that the range spans multiple orders of magnitude¬†</span>
    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">182</span>, <span style="color:#ae81ff">5432</span>, <span style="color:#ae81ff">15329</span>, <span style="color:#ae81ff">252531</span>])

t1, t2 <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">500</span>
img1 <span style="color:#f92672">=</span> capture_image(E<span style="color:#f92672">=</span>get_extreme_sensor_irradiance(), t<span style="color:#f92672">=</span>t1, G<span style="color:#f92672">=</span>linear_crf)
img2 <span style="color:#f92672">=</span> capture_image(E<span style="color:#f92672">=</span>get_extreme_sensor_irradiance(), t<span style="color:#f92672">=</span>t2, G<span style="color:#f92672">=</span>linear_crf)
util<span style="color:#f92672">.</span>display_images([img1, img2], 
                    titles<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;Exposure: t1 = 1/5 second&#34;</span>, 
                            <span style="color:#e6db74">&#34;Exposure: t2 = 1/500 second&#34;</span>])
</code></pre></div><p><img src="/crf/lost-information-example.png#center" alt="Lost Pixel Information"></p>
<p>In this example, it is not possible to capture the scene without loss of information regardless of the chosen exposure time. Correctly exposing the brightest pixel(s) will result in underexposing the darkest pixel(s) and vice versa. Note that in the second image we have one underexposed and one overexposed pixel.</p>
<p>Feel free to <a href="https://github.com/martomi/roboalgorithms-code/blob/main/notebooks/camera-response-function.ipynb">download the
notebook</a> and try it for yourself.</p>
<h1 id="hdr-images-">HDR Images ü™Ç<a hidden class="anchor" aria-hidden="true" href="#hdr-images-">#</a></h1>
<p>The pixel values provide us only partial information due to the limited dynamic range of the camera. To reconstruct an <a href="https://en.wikipedia.org/wiki/High-dynamic-range_imaging">HDR image</a>, we need to recover the full sensor irradiance.</p>
<div class="alert alert-info">
  <div class="alert-icon">üí°</div>
  <div class="alert-content">Remember that outside <a href="https://en.wikipedia.org/wiki/Simulation_hypothesis">our</a> simulation, we can only observe the pixel values from the image and not the actual sensor irradiance.</div>
</div>

<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">We can do that by combining information from multiple images at different exposure times that allow us to expose each
pixel correctly.</div>
</div>

<h2 id="inverse-crf-">Inverse CRF üîô<a hidden class="anchor" aria-hidden="true" href="#inverse-crf-">#</a></h2>
<p>The <a href="https://en.wikipedia.org/wiki/Inverse_function">inverse</a> camera response function allows us to do precisely that. It
defines the mapping from pixel values back to sensor irradiance or light intensity.</p>
<p><img src="/crf/inverse-crf.gif#center" alt="Inverse CRF"></p>
<p>We define <span>
  \(U := G^{-1}\)
</span>
 as the inverse of the CRF. Here we assume that <em>G</em> is <a href="https://en.wikipedia.org/wiki/Monotonic_function">monotonically</a> increasing and thus <a href="https://en.wikipedia.org/wiki/Inverse_function">invertible</a>. That&rsquo;s a fair assumption because cameras are mapping brighter scene regions to brighter image intensities.</p>
<p>Remember that we defined taking an image as <span>
  \(I(k) = G(t\cdot E(k))\)
</span>
. To go the other direction, we can apply the inverse function <span>
  \(U(I(k)) = t\cdot E(k)\)
</span>
 and we can get the sensor irradiance <em>E</em> from the image <em>I</em> if we know the exposure time <em>t</em>:</p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">$$E(k) = \frac{U(I(k))}{t}$$</div>
</div>

<p>The inverse CRF is also <a href="https://en.wikipedia.org/wiki/Identity_function">identity function</a> <span>
  \(f(x) = x\)
</span>
 because:</p>
<p>$$U(G(x)) = x$$</p>
<h2 id="recover-image-irradiance-">Recover Image Irradiance üñºÔ∏è<a hidden class="anchor" aria-hidden="true" href="#recover-image-irradiance-">#</a></h2>
<div class="alert alert-info">
  <div class="alert-icon">üí°</div>
  <div class="alert-content">I am using image irradiance to refer to the (partial) sensor irradiance recovered from a <em>single</em> image. This may differ from usage in the literature.</div>
</div>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">InverseResponseFunction <span style="color:#f92672">=</span> Callable[[np<span style="color:#f92672">.</span>ndarray], np<span style="color:#f92672">.</span>ndarray]

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">recover_image_irradiance</span>(I: np<span style="color:#f92672">.</span>ndarray, 
                             t: float, U: InverseResponseFunction) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
    <span style="color:#e6db74">&#34;&#34;&#34;Recover the irradiance from an image that was
</span><span style="color:#e6db74">    taken with known exposure time t.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    We cannot recover any useful information from 
</span><span style="color:#e6db74">    oversaturated (255) or undersaturated (0) image 
</span><span style="color:#e6db74">    intensities. Return 0 in both cases as this helps
</span><span style="color:#e6db74">    eliminate the need for separate case handling.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">    I  -- image (array of image intensities)
</span><span style="color:#e6db74">    t  -- exposure time (seconds)
</span><span style="color:#e6db74">    U  -- inverse of the response function
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    I[I <span style="color:#f92672">==</span> <span style="color:#ae81ff">255</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span> <span style="color:#75715e"># oversaturated pixels</span>
    <span style="color:#66d9ef">return</span> U(I) <span style="color:#f92672">/</span> t

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">linear_inverse_crf</span>(I: np<span style="color:#f92672">.</span>ndarray) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
    <span style="color:#e6db74">&#34;&#34;&#34;Linear inverse camera response function U(x) = x&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">return</span> I
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">print</span>(f<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Image irradiance (t = {t1}):&#34;</span>)
<span style="color:#66d9ef">print</span>(recover_image_irradiance(I<span style="color:#f92672">=</span>img1, t<span style="color:#f92672">=</span>t1, U<span style="color:#f92672">=</span>linear_inverse_crf))
<span style="color:#66d9ef">print</span>(f<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Image irradiance (t = {t2}):&#34;</span>)
<span style="color:#66d9ef">print</span>(recover_image_irradiance(I<span style="color:#f92672">=</span>img2, t<span style="color:#f92672">=</span>t2, U<span style="color:#f92672">=</span>linear_inverse_crf))
</code></pre></div>










  





  


<blockquote>
  <p><p>Image irradiance (t = 0.2):
[185.   0.   0.   0.]</p>
<p>Image irradiance (t = 0.002):
[    0.  6000. 15000.     0.]</p>
</p>
  <footer>
    <strong></strong>
    
      
        
      
    
  </footer>
</blockquote>

<p>Notice that we can recover the irradiance only for pixels that are not under- or overexposed.
To recover the full sensor irradiance, we need to capture a series of images with different exposure times such that each pixel was correctly exposed at least once.</p>
<p>Let&rsquo;s take another image with a shorter exposure time:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">t3 <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">2000</span>
img3 <span style="color:#f92672">=</span> capture_image(E<span style="color:#f92672">=</span>get_extreme_sensor_irradiance(), t<span style="color:#f92672">=</span>t3, G<span style="color:#f92672">=</span>linear_crf)
<span style="color:#66d9ef">print</span>(f<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Image irradiance (t = {t3:.4f}):&#34;</span>)
<span style="color:#66d9ef">print</span>(recover_image_irradiance(I<span style="color:#f92672">=</span>img3, t<span style="color:#f92672">=</span>t3, U<span style="color:#f92672">=</span>linear_inverse_crf))
</code></pre></div>










  





  


<blockquote>
  <p>Image irradiance (t = 0.0005):
[     0.   6000.  16000. 254000.]</p>
  <footer>
    <strong></strong>
    
      
        
      
    
  </footer>
</blockquote>

<h2 id="recover-sensor-irradiance-">Recover Sensor Irradiance üì∑<a hidden class="anchor" aria-hidden="true" href="#recover-sensor-irradiance-">#</a></h2>
<p>We can now estimate the full sensor irradiance by combining the individual image irradiances. Notice that the second
and the third image have a conflicting estimate of the third pixel value (15000 vs 16000). We&rsquo;re going to take the average.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> typing <span style="color:#f92672">import</span> List

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">estimate_sensor_irradiance_average</span>(images: List[np<span style="color:#f92672">.</span>ndarray], 
                                       exposures: List[float], 
                                       U: InverseResponseFunction) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
    <span style="color:#e6db74">&#34;&#34;&#34;Recover the sensor irradiance
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    images    -- list of images to use for recovery
</span><span style="color:#e6db74">    exposures -- corresponding list of exposures
</span><span style="color:#e6db74">    U         -- inverse response function
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">assert</span> len(images) <span style="color:#f92672">==</span> len(exposures), \
            <span style="color:#e6db74">&#34;The number of images and exposures don&#39;t match&#34;</span>
    <span style="color:#66d9ef">assert</span> len(images) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#34;Expected non-empty list of images&#34;</span>
    
    image_size <span style="color:#f92672">=</span> images[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
    
    irradiance_sum <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(image_size)
    irradiance_count <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(image_size)
    <span style="color:#66d9ef">for</span> I, t <span style="color:#f92672">in</span> zip(images, exposures):
        image_irradiance <span style="color:#f92672">=</span> recover_image_irradiance(I, t, U)
        irradiance_sum <span style="color:#f92672">+=</span> image_irradiance
        <span style="color:#75715e"># Only non-zero values are observed and contribute to average</span>
        irradiance_count[image_irradiance <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>

    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>divide(irradiance_sum, irradiance_count, 
                     out<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>zeros(image_size), 
                     where<span style="color:#f92672">=</span>irradiance_count <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>)
</code></pre></div><p>Let&rsquo;s compare that to the known ground-truth.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">estimated_sensor_irradiance <span style="color:#f92672">=</span> estimate_sensor_irradiance_average(
    [img1, img2, img3], [t1, t2, t3], U<span style="color:#f92672">=</span>linear_inverse_crf)

util<span style="color:#f92672">.</span>print_error_to_ground_truth(
    estimated_sensor_irradiance, ground_truth<span style="color:#f92672">=</span>get_extreme_sensor_irradiance())
</code></pre></div>










  





  


<blockquote>
  <p><p>Ground-truth:  [   182   5432  15329 252531]</p>
<p>Estimated:  [   180.   6750.  13750. 248000.]</p>
<p>Error (diff):  [    2. -1318.  1579.  4531.]</p>
</p>
  <footer>
    <strong></strong>
    
      
        
      
    
  </footer>
</blockquote>

<p>It seems we&rsquo;re close but have lost some precision due to quantization effects and image noise. Can we do better? Maybe average over more images?</p>
<h3 id="improving-the-sensor-irradiance-estimate">Improving the sensor irradiance estimate<a hidden class="anchor" aria-hidden="true" href="#improving-the-sensor-irradiance-estimate">#</a></h3>
<p>In this section, we run several experiments to better understand how the noise affects the accuracy. First,
we need a more representative dataset with more images.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">from</span> typing <span style="color:#f92672">import</span> Tuple, List

ImagesExposuresTuple <span style="color:#f92672">=</span> Tuple[List[np<span style="color:#f92672">.</span>ndarray], List[float]]

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">generate_sliding_exposure_dataset</span>(
    E: np<span style="color:#f92672">.</span>ndarray, G: ResponseFunction, bit_depth: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">8</span>, 
    start_us: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">40</span>, step_perc: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>, number_exposures: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">220</span>, 
    images_per_exposure: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">-&gt;</span> ImagesExposuresTuple:
    <span style="color:#e6db74">&#34;&#34;&#34;Generate dataset with sliding exposure values
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    E                    -- sensor irradiance
</span><span style="color:#e6db74">    G                    -- camera response function
</span><span style="color:#e6db74">    bit_depth            -- bit depth for the camera&#39;s internal processing
</span><span style="color:#e6db74">    start_us             -- microseconds for the starting value of the sequence
</span><span style="color:#e6db74">    step_perc            -- percentage step between subsequent exposure values
</span><span style="color:#e6db74">    number_exposures     -- number of unique exposure times to generate
</span><span style="color:#e6db74">    images_per_exposure  -- number of images per exposure time
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_exposure_seconds</span>(idx: int) <span style="color:#f92672">-&gt;</span> float:
        multiplier <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> (step_perc <span style="color:#f92672">/</span> <span style="color:#ae81ff">100</span>)) <span style="color:#f92672">**</span> idx
        <span style="color:#66d9ef">return</span> round(start_us <span style="color:#f92672">*</span> multiplier) <span style="color:#f92672">*</span> <span style="color:#ae81ff">1e-6</span>
    
    images, exposures <span style="color:#f92672">=</span> [], []
    <span style="color:#66d9ef">for</span> exposure_idx <span style="color:#f92672">in</span> range(number_exposures):
        <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(images_per_exposure):
            t <span style="color:#f92672">=</span> get_exposure_seconds(exposure_idx)
            images<span style="color:#f92672">.</span>append(capture_image(E<span style="color:#f92672">=</span>E, t<span style="color:#f92672">=</span>t, G<span style="color:#f92672">=</span>G))
            exposures<span style="color:#f92672">.</span>append(t)
            
    <span style="color:#66d9ef">return</span> images, exposures
</code></pre></div><p>Feel free to experiment with the parameters for the dataset generation and see what the impact on the final sensor irradiance estimates is.</p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">We can verify that the dataset is statistically well distributed by plotting the number of well-exposed observations for each of the pixels in the image.</div>
</div>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">images, exposures <span style="color:#f92672">=</span> generate_sliding_exposure_dataset(
            E<span style="color:#f92672">=</span>get_extreme_sensor_irradiance(), G<span style="color:#f92672">=</span>linear_crf)

util<span style="color:#f92672">.</span>display_valid_observations(np<span style="color:#f92672">.</span>array(images))
</code></pre></div><p><img src="/crf/valid-observation-stats.png#center" alt="Valid Observation Stats"></p>
<p>Following, I define a helper function that will help us quickly run a few experiments.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">EstimateSensorIrradianceFunction <span style="color:#f92672">=</span> Callable[[List[np<span style="color:#f92672">.</span>ndarray], List[float], 
                                             InverseResponseFunction], np<span style="color:#f92672">.</span>ndarray]

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_irradiance_estimates</span>(
    estimate_func: EstimateSensorIrradianceFunction, 
    images_per_exposure: int, 
    number_experiments: int) <span style="color:#f92672">-&gt;</span> List[np<span style="color:#f92672">.</span>ndarray]:
    <span style="color:#e6db74">&#34;&#34;&#34;Get sensor irradiance estimates from a number of experiments&#34;&#34;&#34;</span>
    
    estimates <span style="color:#f92672">=</span> []

    <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(number_experiments):
        images, exposures <span style="color:#f92672">=</span> generate_sliding_exposure_dataset(
            E<span style="color:#f92672">=</span>get_extreme_sensor_irradiance(), G<span style="color:#f92672">=</span>linear_crf,
            images_per_exposure<span style="color:#f92672">=</span>images_per_exposure)
        irradiance_estimate <span style="color:#f92672">=</span> estimate_func(
            images, exposures, U<span style="color:#f92672">=</span>linear_inverse_crf)
        estimates<span style="color:#f92672">.</span>append(irradiance_estimate)

    <span style="color:#66d9ef">return</span> estimates
</code></pre></div><p>Let&rsquo;s run 10 experiments with a single image per exposure time.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">irradiance_estimates <span style="color:#f92672">=</span> get_irradiance_estimates(
    estimate_sensor_irradiance_average, 
    images_per_exposure<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, number_experiments<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
util<span style="color:#f92672">.</span>plot_errors_to_ground_truth(
    irradiance_estimates, ground_truth<span style="color:#f92672">=</span>get_extreme_sensor_irradiance())
</code></pre></div><p>










  





  


<blockquote>
  <p>RMSE:  1425.4632661240885</p>
  <footer>
    <strong></strong>
    
      
        
      
    
  </footer>
</blockquote>

<img src="/crf/experiments-without-repeated-exposure.png#center" alt="Experiments Without Repeated Exposure"></p>
<p>Now we have a baseline of <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">root-mean-square error</a> (RMSE) that we want to minimize.
I&rsquo;ve also plotted the per-pixel RMSE and the per-experiment absolute error for the estimate.</p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">We can smoothen the noise variations between the experiments by averaging over multiple images with the same exposure.</div>
</div>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">rradiance_estimates <span style="color:#f92672">=</span> get_irradiance_estimates(
    estimate_sensor_irradiance_average, 
<span style="display:block;width:100%;background-color:#3c3d38">    images_per_exposure<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, 
</span>    number_experiments<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
util<span style="color:#f92672">.</span>plot_errors_to_ground_truth(
    irradiance_estimates, ground_truth<span style="color:#f92672">=</span>get_extreme_sensor_irradiance())</code></pre></div>
<p>










  





  


<blockquote>
  <p>RMSE:  1215.9117041699783</p>
  <footer>
    <strong></strong>
    
      
        
      
    
  </footer>
</blockquote>

<img src="/crf/experiments-with-repeated-exposure.png#center" alt="Experiments With Repeated Exposure"></p>
<p>The vertical oscillations have now subsided and the RMSE is slightly improved.</p>
<p>Notice that the errors for most pixels, except the fourth (red in the figure), are biased toward negative
values. This is not caused by the image noise because we would otherwise expect a mean error of zero.</p>
<p>Let&rsquo;s plot the distribution of the observations that we have for each pixel.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">util<span style="color:#f92672">.</span>display_observations_distribution(np<span style="color:#f92672">.</span>array(images))
</code></pre></div><p><img src="/crf/pixel-observations-distribution.png#center" alt="Distribution of per-pixel value observations"></p>
<p>Now we see that the observations in our dataset are similarly skewed. The fourth pixel which had the smallest
bias in the estimate also has the best observation distribution.</p>
<p><img src="/crf/the-thing-about-data-is-garbage-in-garbage-out.jpg#center" alt="What you put in is what you get out"></p>
<p>The skewed observations are the result of using a <a href="https://en.wikipedia.org/wiki/Geometric_progression">geometric sequence</a> for the
exposure times. In our case it looks like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">util<span style="color:#f92672">.</span>plot_exposures(exposures)
</code></pre></div><p><img src="/crf/geometric-exposure-sequence.png#center" alt="Geometric exposure sequence"></p>
<p>The geometric progression is convenient here due to the large range of values we need to cover. We are able to do so
with just over 200 exposures. If we use an <a href="https://en.wikipedia.org/wiki/Arithmetic_progression">arithmetic progression</a>,
on the other hand, we would need significantly more pictures in the dataset which is not always practical or possible.</p>
<p>Instead, we can improve the algorithm by adding a weighting function that discounts the contribution from observations with lower and higher pixel values.</p>
<p>I&rsquo;m going to use the weighting function proposed by <a href="https://people.eecs.berkeley.edu/~malik/papers/debevec-malik97.pdf">Debevek &amp; Malik</a>:</p>
<span>
  \[w(z) = \begin{cases}
        z - Z_{min} &amp; \text{for } z\leq \frac{1}{2}(Z_{min} &#43; Z_{max})\\
        Z_{max} - z &amp; \text{for } z\gt \frac{1}{2}(Z_{min} &#43; Z_{max})
        \end{cases}\]
</span>

<p>where <span>
  \(Z_{min}\)
</span>
 and <span>
  \(Z_{max}\)
</span>
 are the least and greatest pixel values ‚Äî 0 and 255 respectively.</p>
<span>
  \[w(z) = \begin{cases}
        z &amp; \text{for } z\leq 127\\
        255 - z &amp; \text{for } z\gt 127
        \end{cases}\]
</span>

<p>This function discounts both lower and higher pixel values. Besides the higher robustness towards <a href="https://www.researchgate.net/publication/228612445_A_model_for_measurement_of_noise_in_CCD_digital-video_cameras">different noise sources</a> (see also <a href="https://hamamatsu.magnet.fsu.edu/articles/ccdsatandblooming.html">CCD Saturation and Blooming</a>), it also reflects the fact that values in the mid-range are most accurately encoded by non-linear camera response functions (more on this later).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">weighting_debevek_malik</span>(pix_value: int) <span style="color:#f92672">-&gt;</span> int:
    <span style="color:#e6db74">&#34;&#34;&#34;Weighting function for recovering irradiance&#34;&#34;&#34;</span>
    <span style="color:#66d9ef">if</span> pix_value <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">127</span>:
        <span style="color:#66d9ef">return</span> pix_value
    <span style="color:#66d9ef">if</span> pix_value <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">127</span>:
        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">255</span> <span style="color:#f92672">-</span> pix_value
</code></pre></div><p><img src="/crf/weight-function.png#center" alt="Weighting Function"></p>
<p>Let&rsquo;s rewrite the function for sensor irradiance estimation to use the weighting function:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">estimate_sensor_irradiance_weighted</span>(
    images: List[np<span style="color:#f92672">.</span>ndarray], exposures: List[float], 
    U: Callable[[np<span style="color:#f92672">.</span>ndarray], np<span style="color:#f92672">.</span>ndarray], w: Callable[[int], int]) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
    <span style="color:#e6db74">&#34;&#34;&#34;Estimate the sensor irradiance
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    The recovered image irradiances are weighted according to the provided
</span><span style="color:#e6db74">    weighting function. This works best with higher number of images 
</span><span style="color:#e6db74">    and well-distributed exposure times.
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    images    -- list of images to use for recovery
</span><span style="color:#e6db74">    exposures -- corresponding list of exposures
</span><span style="color:#e6db74">    U         -- inverse response function
</span><span style="color:#e6db74">    w         -- weighting function
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">assert</span> len(images) <span style="color:#f92672">==</span> len(exposures), \
            <span style="color:#e6db74">&#34;The number of images and exposures don&#39;t match&#34;</span>
    <span style="color:#66d9ef">assert</span> len(images) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#34;Expected non-empty list of images&#34;</span>
    
    image_size <span style="color:#f92672">=</span> images[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
    w_vec <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>vectorize(w)
    
    irradiance_sum <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(image_size)
    irradiance_count <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(image_size)
    <span style="color:#66d9ef">for</span> I, t <span style="color:#f92672">in</span> zip(images, exposures):
        weights <span style="color:#f92672">=</span> w_vec(I)
        image_irradiance <span style="color:#f92672">=</span> recover_image_irradiance(I, t, U)
        irradiance_sum <span style="color:#f92672">+=</span> weights <span style="color:#f92672">*</span> image_irradiance
        <span style="color:#75715e"># Only non-zero values are observed and contribute to average</span>
        irradiance_count[image_irradiance <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>] <span style="color:#f92672">+=</span> weights[image_irradiance <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>]

    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>divide(irradiance_sum, irradiance_count, 
                     out<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>zeros(image_size), 
                     where<span style="color:#f92672">=</span>irradiance_count <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>)
</code></pre></div><p>










  





  


<blockquote>
  <p>RMSE:  165.4355269685378</p>
  <footer>
    <strong></strong>
    
      
        
      
    
  </footer>
</blockquote>

<img src="/crf/experiments-with-weighting.png#center" alt="Experiments with Weighting Function"></p>
<p>This change alone provides an almost 10x improvement in the estimate errors. I found out that using the square of
the weighting function provides even better results.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">weighting_squared</span>(pix_value: int) <span style="color:#f92672">-&gt;</span> int:
    <span style="color:#66d9ef">return</span> weighting_debevek_malik(pix_value) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>
</code></pre></div><p><img src="/crf/weight-function-squared.png#center" alt="Weighting Function Squared"></p>
<p>










  





  


<blockquote>
  <p>RMSE:  72.49150623466912</p>
  <footer>
    <strong></strong>
    
      
        
      
    
  </footer>
</blockquote>

<img src="/crf/experiments-with-weighting-squared.png#center" alt="Experiments with Weighting Function Squared"></p>
<p>And here&rsquo;s the final estimate:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">images, exposures <span style="color:#f92672">=</span> generate_sliding_exposure_dataset(
            E<span style="color:#f92672">=</span>get_extreme_sensor_irradiance(), G<span style="color:#f92672">=</span>linear_crf)

recovered_sensor_irradiance <span style="color:#f92672">=</span> estimate_sensor_irradiance_weighted(
    images, exposures, U<span style="color:#f92672">=</span>linear_inverse_crf, w<span style="color:#f92672">=</span>weighting_squared)
util<span style="color:#f92672">.</span>print_error_to_ground_truth(
    recovered_sensor_irradiance, ground_truth<span style="color:#f92672">=</span>get_extreme_sensor_irradiance())
</code></pre></div>










  





  


<blockquote>
  <p><p>Ground-truth:  [   182   5432  15329 252531]</p>
<p>Estimated:  [   184.362   5436.691  15341.962 252564.875]</p>
<p>Error (diff):  [ -2.362  -4.691 -12.962 -33.875]</p>
</p>
  <footer>
    <strong></strong>
    
      
        
      
    
  </footer>
</blockquote>

<h2 id="absolute-vs-relative-irradiance-">Absolute vs Relative Irradiance ‚ö†Ô∏è<a hidden class="anchor" aria-hidden="true" href="#absolute-vs-relative-irradiance-">#</a></h2>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">So far we&rsquo;ve successfully recovered the <strong>relative</strong> sensor irradiance values.</div>
</div>

<p>Recovering the <strong>absolute</strong> sensor irradiance isn&rsquo;t quite as straightforward for real cameras. We&rsquo;ve used a very
simplified model of capturing images and haven&rsquo;t taken the photodiode&rsquo;s <a href="https://en.wikipedia.org/wiki/Quantum_efficiency">quantum
efficiency</a> and other physical aspects into consideration.</p>
<p>Although it may seem that we fully recovered the irradiance values, this method can only recover the relative
irradiance values up to an unknown constant factor <span>
  \(X\)
</span>
.</p>
<p>$$ E_{absolute} = X \cdot E_{relative} $$</p>
<p>To determine <span>
  \(X\)
</span>
, an <a href="https://www.spiedigitallibrary.org/journals/journal-of-applied-remote-sensing/volume-5/issue-1/053544/Absolute-radiometric-calibration-of-the-RapidEye-multispectral-imager-using-the/10.1117/1.3613950.short?SSO=1">absolute radiometric
calibration</a>
can be performed. The process involves taking an image of a surface with known reflectance values in a controlled light
environment.</p>
<p>Conveniently, the relative irradiance values that we obtained are sufficient for most applications, including HDR
images.</p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">The irradiance values we recovered are our HDR image!</div>
</div>

<h2 id="displaying-hdr-images-">Displaying HDR Images üñ•Ô∏è<a hidden class="anchor" aria-hidden="true" href="#displaying-hdr-images-">#</a></h2>
<p>We could display the HDR image by normalizing the values within the 0-255 range.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">normalize_255</span>(image: np<span style="color:#f92672">.</span>ndarray) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
    <span style="color:#e6db74">&#34;&#34;&#34;Normalize values withing range [0, 255]&#34;&#34;&#34;</span>
    min_i, max_i <span style="color:#f92672">=</span> min(image), max(image)
    <span style="color:#66d9ef">assert</span> min_i <span style="color:#f92672">&lt;</span> max_i, <span style="color:#e6db74">&#34;Range cannot be normalized (min == max)&#34;</span>
    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>around((image <span style="color:#f92672">-</span> min_i) <span style="color:#f92672">*</span> (<span style="color:#ae81ff">255</span> <span style="color:#f92672">/</span> (max_i <span style="color:#f92672">-</span> min_i)))

hdr_linear <span style="color:#f92672">=</span> normalize_255(recovered_sensor_irradiance)
util<span style="color:#f92672">.</span>display_images([hdr_linear], titles<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;HDR (linear)&#34;</span>])
</code></pre></div><p><img src="/crf/hdr-linear.png#center" alt="HDR Linear"></p>
<p>After normalization, the darkest and brightest pixels are pinned to the values of 0 and 255 respectively. All other values are linearly adjusted. The problem with this method is that usually the scene will have very few but extremely bright spots and this would cause the majority of the rest of the scene to appear underexposed.</p>
<p>The image above is a good example of this effect.</p>
<p>This is a limitation of the display technology today. Most devices still have a non-HDR display which is only capable of displaying <a href="https://en.wikipedia.org/wiki/Color_depth#True_color_(24-bit)">24-bit color</a> which is exactly 8-bit per RGB channel. This sets a hard limit on the range of values [0-255].</p>
<h2 id="gamma-encoding-">Gamma encoding üìà<a hidden class="anchor" aria-hidden="true" href="#gamma-encoding-">#</a></h2>
<p>To go around this problem cameras perform <a href="https://en.wikipedia.org/wiki/Gamma_correction">gamma encoding</a> when saving images in 8-bit format. In doing so, the limited range can be utilized more efficiently.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gamma</span>(X: np<span style="color:#f92672">.</span>ndarray) <span style="color:#f92672">-&gt;</span> np<span style="color:#f92672">.</span>ndarray:
    <span style="color:#66d9ef">assert</span> all(X <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">0</span>) <span style="color:#f92672">and</span> all(X <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1</span>)
    <span style="color:#66d9ef">return</span> X<span style="color:#f92672">**</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">2.2</span>)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_gamma_function</span>():
    plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Linear vs Gamma&#34;</span>)
    plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Input&#34;</span>)
    plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Output&#34;</span>)

    x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0.001</span>)
    plt<span style="color:#f92672">.</span>plot(x, gamma(x))
    plt<span style="color:#f92672">.</span>plot(x, x)
    
plot_gamma_function()
</code></pre></div><p><img src="/crf/linear-vs-gamma.png#center" alt="Linear vs Gamma functions"></p>
<p>The gamma function encodes lower values (darker tones) with significantly higher resolution. This mapping exploits the
fact that our eyes perceive brightness proportionally to the logarithm of the light intensity. See <a href="https://en.wikipedia.org/wiki/Weber%E2%80%93Fechner_law">Weber-Fechner
law</a>.</p>
<p>Think of a light bulb with adjustable brightness. If we increase the light intensity by N two consecutive times, the
<em>perceived</em> brightness change will be larger the first time.</p>
<p><img src="/crf/linear-light.png#center" alt="Linear light increase"></p>
<p>The gamma encoding is reversed by a gamma decoding step which applies the inverse of the gamma function. Your monitor
<a href="http://www.lagom.nl/lcd-test/gamma_calibration.php">likely expects</a> gamma encoded images and automatically does the
decoding. All of this is formally defined in the <a href="https://en.wikipedia.org/wiki/SRGB">sRGB color space</a>.</p>
<p>Note that the gamma function is just one of many possible <a href="https://en.wikipedia.org/wiki/Tone_mapping">tone mapping</a>
techniques. We are not going to cover more, but I highly recommend this great article on <a href="https://ciechanow.ski/color-spaces/">color
spaces</a>.</p>
<p>For very high dynamic range images you can also <a href="http://resources.mpi-inf.mpg.de/tmo/logmap/logmap.pdf">take the
logarithm</a> and normalize the resulting values over the [0-255]
range. Following is an example of both approaches.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">apply_gamma</span>(E: np<span style="color:#f92672">.</span>ndarray, bit_depth: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">8</span>):
    <span style="color:#66d9ef">return</span> gamma(E <span style="color:#f92672">/</span> max_value(bit_depth)) <span style="color:#f92672">*</span> max_value(bit_depth)

hdr_gamma <span style="color:#f92672">=</span> apply_gamma(normalize_255(recovered_sensor_irradiance))
hdr_log <span style="color:#f92672">=</span> normalize_255(np<span style="color:#f92672">.</span>log(recovered_sensor_irradiance <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>))

util<span style="color:#f92672">.</span>display_images([hdr_linear, hdr_gamma, hdr_log], titles<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;HDR (linear)&#34;</span>, <span style="color:#e6db74">&#34;HDR (gamma)&#34;</span>, <span style="color:#e6db74">&#34;HDR (log)&#34;</span>])
</code></pre></div><p><img src="/crf/hdr-display-alternatives.png#center" alt="HDR Display alternatives"></p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">The darker values are now much better represented and we can easily identify the nuances.</div>
</div>

<h1 id="non-linear-crf-">Non-linear CRF üéõÔ∏è<a hidden class="anchor" aria-hidden="true" href="#non-linear-crf-">#</a></h1>
<p>Up to now, we&rsquo;ve assumed that the CRF is linear <span>
  \(G(x) = x\)
</span>
. However, we&rsquo;ve seen that it makes sense for cameras to encode images using a non-linear curve to optimize for dynamic range and appearance.</p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">This can result in <em>different</em> pixel values for the <em>same</em> light intensity.</div>
</div>

<p><img src="/crf/crf-animation-2.gif#center" alt="CRF Animation"></p>
<p>Additional non-linearities might be introduced by other parts of the system ‚Äî e.g. the <a href="https://en.wikipedia.org/wiki/Analog-to-digital_converter#Nonlinearity">A/D converter</a> or other digital image processing steps such as <a href="https://en.wikipedia.org/wiki/Color_balance">white balance</a> correction. The goal of the CRF calibration is to find the overall response of the camera that combines all these factors into a single function.</p>
<p><img src="/crf/crf-function.gif#center" alt="CRF Function"></p>
<h1 id="crf-calibration-">CRF Calibration üî¨<a hidden class="anchor" aria-hidden="true" href="#crf-calibration-">#</a></h1>
<p>In this section, we&rsquo;re going to implement a simple but effective algorithm for determining the camera response function from a set of monochrome images. The algorithm is inspired by the approach presented in the paper: <a href="https://arxiv.org/abs/1607.02555">A Photometrically Calibrated Benchmark For Monocular Visual Odometry</a>.</p>
<h2 id="calibrate-from-a-single-pixel-">Calibrate from a single pixel üí°<a hidden class="anchor" aria-hidden="true" href="#calibrate-from-a-single-pixel-">#</a></h2>
<p>Let&rsquo;s iteratively discover the approach by first considering an image consisting of 1 pixel.</p>
<p><img src="/crf/inverse-crf.gif#center" alt="Inverse CRF"></p>
<p>Given our assumptions about the scene, we have full control over the amount of light that a pixel receives by controlling the exposure time. We don&rsquo;t know the absolute value but we know that if we double the exposure time, we receive double the amount of light.</p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">If we correlate that known light change with the resulting pixel value, we can determine the function that relates them.</div>
</div>

<p>The inverse CRF takes a pixel value as an argument and we know that pixel values are integers in the range [0-255]. So we can define it as a <a href="https://www.sciencedirect.com/topics/computer-science/discrete-function">discrete function</a>. In Python, this translates to an array where the index of the array is the input to the function.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">U <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.</span>] <span style="color:#f92672">*</span> <span style="color:#ae81ff">256</span>)
</code></pre></div><p>We need to have observations for each possible pixel value to fully determine the inverse CRF. So our
calibration dataset needs to start from a fully underexposed image and iteratively increase
exposure time until the image is overexposed.</p>
<p>Here it makes more sense to use arithmetic progression for the exposures so that we have dense coverage over the
entire range.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">generate_crf_calibration_dataset</span>(
    E: np<span style="color:#f92672">.</span>ndarray, G: Callable[[np<span style="color:#f92672">.</span>ndarray, int], np<span style="color:#f92672">.</span>ndarray], 
    bit_depth: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">8</span>, start_ms: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, end_ms: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">201</span>, 
    step_ms: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, images_per_exposure: int <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">-&gt;</span> ImagesExposuresTuple:
    <span style="color:#e6db74">&#34;&#34;&#34;Generate dataset with sliding exposure values in the specified range
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    E                    -- sensor irradiance
</span><span style="color:#e6db74">    G                    -- camera response function
</span><span style="color:#e6db74">    bit_depth            -- bit depth for the A/D converter of the camera
</span><span style="color:#e6db74">    start_ms             -- start value (ms) for the range of exposure values
</span><span style="color:#e6db74">    end_ms               -- end value (ms) for the range of exposure values
</span><span style="color:#e6db74">    step_ms              -- step size (ms) for the range of exposure values
</span><span style="color:#e6db74">    images_per_exposure  -- number of images per exposure time
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    
    exposures <span style="color:#f92672">=</span> [x <span style="color:#f92672">/</span> <span style="color:#ae81ff">1e3</span> <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> range(start_ms, end_ms, step_ms)]
    exposures <span style="color:#f92672">*=</span> images_per_exposure

    images <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> exposures:
        images<span style="color:#f92672">.</span>append(capture_image(E<span style="color:#f92672">=</span>E, t<span style="color:#f92672">=</span>t, G<span style="color:#f92672">=</span>G, bit_depth<span style="color:#f92672">=</span>bit_depth))
    
    <span style="color:#66d9ef">return</span> images, exposures
</code></pre></div><h3 id="recovering-linear-response">Recovering linear response<a hidden class="anchor" aria-hidden="true" href="#recovering-linear-response">#</a></h3>
<p>As a first example, we&rsquo;re going to generate a dataset using the linear response function and attempt to recover it.
We use an arbitrarily chosen pixel irradiance:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">single_pixel_irradiance <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">1381</span>])
</code></pre></div><p>Generate the dataset and verify that it provides a good distribution of pixel values.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">images, exposures <span style="color:#f92672">=</span> generate_crf_calibration_dataset(
    E<span style="color:#f92672">=</span>single_pixel_irradiance, G<span style="color:#f92672">=</span>linear_crf)
util<span style="color:#f92672">.</span>display_images(images[<span style="color:#ae81ff">0</span>::<span style="color:#ae81ff">10</span>], shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>), annot<span style="color:#f92672">=</span>False)
</code></pre></div><p><img src="/crf/single-pixel-calibration-dataset.png#center" alt="Single pixel dataset"></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">util<span style="color:#f92672">.</span>display_observations_distribution(np<span style="color:#f92672">.</span>array(images))
</code></pre></div><p><img src="/crf/single-pixel-observation-distribution.png#center" alt="Single pixel observation distribution"></p>
<p>We don&rsquo;t have sufficient images to cover <em>every</em> value from the range (200 images vs 256 values) but we have good
enough coverage. Reality isn&rsquo;t perfect either so let&rsquo;s try to work around that and recover the reverse CRF function
<strong>U</strong>.</p>
<p>$$U(I(x)) = t\cdot E(x)$$</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">recover_U_single_pixel</span>(exposures, images):
    <span style="color:#75715e"># Define an arbitrary (relative) sensor irradiance </span>
    <span style="color:#75715e"># Expected to be wrong by a constant factor X</span>
    sensor_irradiance <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">100</span>])
    
    <span style="color:#75715e"># Initialize the inverse function U to 0s</span>
    U <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.</span>] <span style="color:#f92672">*</span> <span style="color:#ae81ff">256</span>)

    <span style="color:#66d9ef">for</span> t, image <span style="color:#f92672">in</span> zip(exposures, images):
        pixel_value <span style="color:#f92672">=</span> int(image[<span style="color:#ae81ff">0</span>])
        <span style="color:#66d9ef">if</span> pixel_value <span style="color:#f92672">==</span> <span style="color:#ae81ff">255</span> <span style="color:#f92672">or</span> pixel_value <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#75715e"># Skip under- or overexposed pixels</span>
            <span style="color:#66d9ef">continue</span>
        accumulated_photons <span style="color:#f92672">=</span> t <span style="color:#f92672">*</span> sensor_irradiance[<span style="color:#ae81ff">0</span>]
        U[pixel_value] <span style="color:#f92672">=</span> accumulated_photons
    
    <span style="color:#66d9ef">return</span> U

util<span style="color:#f92672">.</span>plot_inverse_crf(recover_U_single_pixel(exposures, images))
</code></pre></div><p><img src="/crf/recovered-inverse-crf.png#center" alt="Recovered inverse CRF"></p>
<p>The zero values are caused by unobserved pixel values. One way we can resolve that is by taking more images with finer
exposure steps to cover the entire range of values.</p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">We&rsquo;re going to interpolate the missing values from their nearest neighbors.</div>
</div>

<p>Then we get a very good approximation without stricter requirements on the dataset.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">interpolate_missing_values</span>(U):
    observed_intensity <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(len(U), dtype<span style="color:#f92672">=</span>bool)
    observed_intensity[U <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0.0</span>] <span style="color:#f92672">=</span> True
    U[<span style="color:#f92672">~</span>observed_intensity] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>interp(
        (<span style="color:#f92672">~</span>observed_intensity)<span style="color:#f92672">.</span>nonzero()[<span style="color:#ae81ff">0</span>], 
        observed_intensity<span style="color:#f92672">.</span>nonzero()[<span style="color:#ae81ff">0</span>], 
        U[observed_intensity])
    <span style="color:#66d9ef">return</span> U

util<span style="color:#f92672">.</span>plot_inverse_crf(
    interpolate_missing_values(
        recover_U_single_pixel(exposures, images)))
</code></pre></div><p><img src="/crf/interpolated-inverse-crf.png#center" alt="Interpolated inverse CRF"></p>
<p>The recovered function looks a bit noisy. Debevek &amp; Malik
<a href="https://people.eecs.berkeley.edu/~malik/papers/debevec-malik97.pdf">propose</a> adding a smoothness term based on the sum
of squared second derivatives.</p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">We&rsquo;re going to follow a simpler approach of averaging the values over multiple images from the same exposure.</div>
</div>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># Create a dataset with multiple images for each exposure time</span>
images, exposures <span style="color:#f92672">=</span> generate_crf_calibration_dataset(
    E<span style="color:#f92672">=</span>single_pixel_irradiance, G<span style="color:#f92672">=</span>linear_crf, images_per_exposure<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">recover_U_single_pixel_average</span>(exposures, images):
    sensor_irradiance <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">100</span>])
    
    U_sum <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.</span>] <span style="color:#f92672">*</span> <span style="color:#ae81ff">256</span>)
    U_count <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0</span>] <span style="color:#f92672">*</span> <span style="color:#ae81ff">256</span>)

    <span style="color:#66d9ef">for</span> t, image <span style="color:#f92672">in</span> zip(exposures, images):
        pixel_value <span style="color:#f92672">=</span> int(image[<span style="color:#ae81ff">0</span>])
        <span style="color:#66d9ef">if</span> pixel_value <span style="color:#f92672">==</span> <span style="color:#ae81ff">255</span> <span style="color:#f92672">or</span> pixel_value <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            <span style="color:#75715e"># Skip under- or overexposed pixels</span>
            <span style="color:#66d9ef">continue</span>
        accumulated_photons <span style="color:#f92672">=</span> t <span style="color:#f92672">*</span> sensor_irradiance[<span style="color:#ae81ff">0</span>]
        
        <span style="color:#75715e"># Sum all observations for specific pixel value</span>
        U_sum[pixel_value] <span style="color:#f92672">+=</span> accumulated_photons
        U_count[pixel_value] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
      
    <span style="color:#75715e"># Calculate the average </span>
    observed_intensity <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(len(U_sum), dtype<span style="color:#f92672">=</span>bool)
    observed_intensity[U_count <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> True
    
    <span style="color:#66d9ef">return</span> np<span style="color:#f92672">.</span>divide(U_sum, U_count, out<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>zeros_like(U_sum), 
                     where<span style="color:#f92672">=</span>observed_intensity)

recovered_inverse_crf_linear <span style="color:#f92672">=</span> interpolate_missing_values(
    recover_U_single_pixel_average(exposures, images))
util<span style="color:#f92672">.</span>plot_inverse_crf(recovered_inverse_crf_linear)
</code></pre></div><p><img src="/crf/smoothened-inverse-crf.png#center" alt="Smoothened inverse CRF"></p>
<p>Nice and smooth üòä I call this a success. Let&rsquo;s move onto more interesting examples!</p>
<h3 id="recovering-gamma-response">Recovering gamma response<a hidden class="anchor" aria-hidden="true" href="#recovering-gamma-response">#</a></h3>
<p>To have meaningful gamma encoding, we need to quantize the images with a 12-bit A/D converter which provides a higher dynamic range that we can then effectively compress down to 8-bit with the gamma encoding. Note that we use 12-bit depth for the camera
internal processing but the result is still an 8-bit (gamma encoded) image.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">images, exposures <span style="color:#f92672">=</span> generate_crf_calibration_dataset(
    E<span style="color:#f92672">=</span>single_pixel_irradiance, G<span style="color:#f92672">=</span>apply_gamma, images_per_exposure<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>, bit_depth<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>,
    start_ms<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, end_ms<span style="color:#f92672">=</span><span style="color:#ae81ff">3001</span>, step_ms<span style="color:#f92672">=</span><span style="color:#ae81ff">15</span>)
</code></pre></div><p>Note that I also had to increase the step size for the exposure sequence to cover the extended dynamic range
of the 12-bit A/D converter while keeping the total number of exposures constant between the examples.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">util<span style="color:#f92672">.</span>display_images(images[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">200</span>:<span style="color:#ae81ff">10</span>], shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>), annot<span style="color:#f92672">=</span>False)
</code></pre></div><p><img src="/crf/single-pixel-gamma-calib-dataset.png#center" alt="Single pixel gamma calibration dataset"></p>
<div class="alert alert-info">
  <div class="alert-icon">üí°</div>
  <div class="alert-content"><p>The gamma dataset (bottom) appears to follow a more linear progression compared to the linear dataset (top). That&rsquo;s because your monitor does gamma decoding and wrongly displays non-gamma encoded images.</p>
<p><img src="/crf/single-pixel-calibration-dataset.png#center" alt="Linear">
<img src="/crf/single-pixel-gamma-calib-dataset.png#center" alt="Gamma"></p>
</div>
</div>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">util<span style="color:#f92672">.</span>display_observations_distribution(np<span style="color:#f92672">.</span>array(images))
</code></pre></div><p><img src="/crf/single-pixel-gamma-distribution.png#center" alt="Single pixel gamma distriubtion"></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">recovered_inverse_crf_gamma <span style="color:#f92672">=</span> interpolate_missing_values(
    recover_U_single_pixel_average(exposures, images))
util<span style="color:#f92672">.</span>plot_inverse_crf(recovered_inverse_crf_gamma)
</code></pre></div><p><img src="/crf/recovered-gamma-inverse-crf.png#center" alt="Recovered gamma inverse CRF"></p>
<div class="alert alert-info">
  <div class="alert-icon">üí°</div>
  <div class="alert-content">Direct comparison of the relative irradiance values on the Y-axis between the gamma and linear (inverse)
response functions reveals the wider range of relative irradiances that the gamma function is capable
of encoding (range 0-300 vs 0-18).</div>
</div>

<p><img src="/crf/linear-vs-gamma-y-axis.png#center" alt="Linear vs gamma"></p>
<h2 id="calibrate-from-multiple-pixels-">Calibrate from multiple pixels üê£<a hidden class="anchor" aria-hidden="true" href="#calibrate-from-multiple-pixels-">#</a></h2>
<p>As demonstrated in the previous section, it is possible to calibrate the camera response function by using a single
pixel. The downside of this approach is that it requires a lot of images to densely cover the entire range of values
and average out the noise ‚Äî we took 20 images @ 200 unique exposures resulting in a total dataset size of
4000 images.</p>
<p>Let&rsquo;s hypothetically consider the other extreme case:</p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">What if we have a dataset consisting of a <em>single</em> image with dimensions 64x64?</div>
</div>

<p>The image would have 4096 pixels in total. That&rsquo;s approximately the same number
of pixels as in our dataset with single pixel images. Assuming there&rsquo;s a good variation in the pixel
irradiances, would this provide us with the same amount of information?</p>
<p>The answer is no because we wouldn&rsquo;t know the relative irradiances between the pixels.</p>
<div class="alert alert-info">
  <div class="alert-icon">üí°</div>
  <div class="alert-content">In the case of a single pixel, we had a single irradiance value and we could calculate the change in
the accumulated light based on the exposure time alone. In order to relate pixel
values in the same image, we need to recover the relative sensor irradiance. We already solved this problem in the HDR
image section.</div>
</div>

<p>The catch is that we already need to know the CRF to recover the irradiance values, but the irradiance values
are needed to recover the CRF. The chicken and the egg puzzle?</p>
<p>In the following equation, both <span>
  \(U(x)\)
</span>
 and <span>
  \(E(x)\)
</span>
 are unknown.</p>
<p>$$U(I(x)) = t\cdot E(x)$$</p>
<p>But we know they&rsquo;re both constant so if fix one and solve for the other, we can iteratively converge to a solution. In
other words, we&rsquo;re going to attempt to evolve the chicken and the egg simultaneously! üê£</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">recover_U</span>(E: np<span style="color:#f92672">.</span>ndarray, exposures: List[float], images: List[np<span style="color:#f92672">.</span>ndarray]):
    <span style="color:#e6db74">&#34;&#34;&#34;Attempt to recover the inverse CRF (U)
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Calculation based on the formula U(I(x)) = t * E(x)
</span><span style="color:#e6db74">    where I(x), t and E(x) are given as arguments.
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    All observations for a given I(x) are summed up and 
</span><span style="color:#e6db74">    the average is calculated. Non-observed values are interpolated.
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    E          -- sensor irradiance
</span><span style="color:#e6db74">    exposures  -- list of exposure times t (seconds)
</span><span style="color:#e6db74">    images     -- images 
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    U_sum <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.</span>] <span style="color:#f92672">*</span> <span style="color:#ae81ff">256</span>)
    U_count <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0</span>] <span style="color:#f92672">*</span> <span style="color:#ae81ff">256</span>)

    <span style="color:#66d9ef">for</span> t, image <span style="color:#f92672">in</span> zip(exposures, images):
        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(image)):
            pixel_value <span style="color:#f92672">=</span> int(image[i])
            <span style="color:#66d9ef">if</span> pixel_value <span style="color:#f92672">==</span> <span style="color:#ae81ff">255</span> <span style="color:#f92672">or</span> pixel_value <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
                <span style="color:#66d9ef">continue</span> <span style="color:#75715e"># Skip under- or overexposed pixels</span>
            accumulated_photons <span style="color:#f92672">=</span> t <span style="color:#f92672">*</span> E[i]

            <span style="color:#75715e"># Sum all observations for specific pixel value</span>
            U_sum[pixel_value] <span style="color:#f92672">+=</span> accumulated_photons
            U_count[pixel_value] <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
      
    <span style="color:#75715e"># Calculate U as the average of all observations</span>
    observed_intensity <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(len(U_sum), dtype<span style="color:#f92672">=</span>bool)
    observed_intensity[U_count <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>] <span style="color:#f92672">=</span> True
    U <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>divide(U_sum, U_count, out<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>zeros_like(U_sum), 
                  where<span style="color:#f92672">=</span>observed_intensity)
    
    <span style="color:#66d9ef">return</span> interpolate_missing_values(U)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">recover_U_and_E</span>(exposures, images, iterations<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>):
    <span style="color:#e6db74">&#34;&#34;&#34;Attempt to solve for both the inverse CRF and the sensor irradiance
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Given a dataset, perform a number of iterations by alternatingly solving 
</span><span style="color:#e6db74">    for U(x) and E(x) to check if they can converge to a solution.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    E <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">100</span>] <span style="color:#f92672">*</span> len(images[<span style="color:#ae81ff">0</span>]))
    U <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.</span>] <span style="color:#f92672">*</span> <span style="color:#ae81ff">256</span>)

    response_function <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> x: U[int(x)]

    U_list <span style="color:#f92672">=</span> []
    E_list <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(iterations):
        U <span style="color:#f92672">=</span> recover_U(E, exposures, images)
        U_list<span style="color:#f92672">.</span>append(U)
        E <span style="color:#f92672">=</span> estimate_sensor_irradiance_weighted(
            images, exposures, 
            U<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>vectorize(response_function), 
            w<span style="color:#f92672">=</span>weighting_debevek_malik)
        E_list<span style="color:#f92672">.</span>append(E)
    
    <span style="color:#66d9ef">return</span> U_list, E_list
</code></pre></div><p>Let&rsquo;s generate the dataset:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">images, exposures <span style="color:#f92672">=</span> generate_crf_calibration_dataset(
    E<span style="color:#f92672">=</span>get_sensor_irradiance(), 
    G<span style="color:#f92672">=</span>apply_gamma, 
    images_per_exposure<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, 
    bit_depth<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>,
    start_ms<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, 
    end_ms<span style="color:#f92672">=</span><span style="color:#ae81ff">3001</span>, 
    step_ms<span style="color:#f92672">=</span><span style="color:#ae81ff">15</span>)
</code></pre></div><p>I&rsquo;ve experimented with the parameters until I got good coverage of pixel values for each pixel in the image. Here are the first ten darkest images from the sequence:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">util<span style="color:#f92672">.</span>display_images(images[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">10</span>], annot<span style="color:#f92672">=</span>False)
</code></pre></div><p><img src="/crf/multi-pixel-dataset-first-ten.png#center" alt="First ten image from multipixel dataset"></p>
<p>And here&rsquo;s a representation of the full (subsampled) dataset:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">util<span style="color:#f92672">.</span>display_images(images[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">200</span>:<span style="color:#ae81ff">10</span>], annot<span style="color:#f92672">=</span>False)
</code></pre></div><p><img src="/crf/multi-pixel-dataset-subsampled.png#center" alt="Subsampled representation of the multipixel dataset"></p>
<p>Now we optimize for both the irradiance image and the inverse response:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">U_list, E_list <span style="color:#f92672">=</span> recover_U_and_E(exposures, images, iterations <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>);
</code></pre></div><p>Here are the inverse response functions after each iteration of the optimization:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">util<span style="color:#f92672">.</span>plot_multiple_inverse_crf(U_list)
</code></pre></div><p><img src="/crf/inverse-response-after-each-iteration-1.png#center" alt="Inverse response after each iteration"></p>
<p>And the irradiance images after each iteration:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">util<span style="color:#f92672">.</span>display_images([normalize_255(np<span style="color:#f92672">.</span>log(E <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)) <span style="color:#66d9ef">for</span> E <span style="color:#f92672">in</span> E_list])
</code></pre></div><p><img src="/crf/irradiance-image-after-each-iteration-1.png#center" alt="Irradiance image after each iteration"></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Normalized Ground-truth: &#34;</span>, normalize_255(get_sensor_irradiance()))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Normalized Estimate: &#34;</span>, normalize_255(E_list[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Error: &#34;</span>, normalize_255(
    get_sensor_irradiance()) <span style="color:#f92672">-</span> normalize_255(E_list[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]))
</code></pre></div>










  





  


<blockquote>
  <p><p>Normalized Ground-truth:  [  0.  23.   4. 255.]</p>
<p>Normalized Estimate:  [  0.  23.   4. 255.]</p>
<p>Error:  [0. 0. 0. 0.]</p>
</p>
  <footer>
    <strong></strong>
    
      
        
      
    
  </footer>
</blockquote>

<p>This approach quickly converges to the correct solution for both U and E. It starts falling apart if we try it with the more extreme sensor irradiance values.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">images, exposures <span style="color:#f92672">=</span> generate_crf_calibration_dataset(
    E<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>array(get_extreme_sensor_irradiance()), G<span style="color:#f92672">=</span>apply_gamma, 
    images_per_exposure<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, bit_depth<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>,
    start_ms<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, end_ms<span style="color:#f92672">=</span><span style="color:#ae81ff">3001</span>, step_ms<span style="color:#f92672">=</span><span style="color:#ae81ff">15</span>)

util<span style="color:#f92672">.</span>display_images(images[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">10</span>], annot<span style="color:#f92672">=</span>False)
util<span style="color:#f92672">.</span>display_images(images[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">200</span>:<span style="color:#ae81ff">10</span>], annot<span style="color:#f92672">=</span>False)
</code></pre></div><p><img src="/crf/multi-pixel-dataset-first-ten-2.png#center" alt="First ten from dataset">
<img src="/crf/multi-pixel-dataset-subsampled-2.png#center" alt="Subsampled dataset"></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">U_list, E_list <span style="color:#f92672">=</span> recover_U_and_E(exposures, images, iterations<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>);
util<span style="color:#f92672">.</span>plot_multiple_inverse_crf(U_list)
util<span style="color:#f92672">.</span>display_images([normalize_255(np<span style="color:#f92672">.</span>log(E <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)) <span style="color:#66d9ef">for</span> E <span style="color:#f92672">in</span> E_list])
</code></pre></div><p><img src="/crf/inverse-response-after-each-iteration-2.png#center" alt="Inverse response after each iteration">
<img src="/crf/irradiance-image-after-each-iteration-2.png#center" alt="Irradiance image after each iteration"></p>
<p>Nothing we can&rsquo;t resolve by throwing 50 iterations at it üòâ</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">U_list, E_list <span style="color:#f92672">=</span> recover_U_and_E(exposures, images, iterations<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>);
util<span style="color:#f92672">.</span>plot_multiple_inverse_crf(U_list[::<span style="color:#ae81ff">10</span>])
util<span style="color:#f92672">.</span>display_images([normalize_255(np<span style="color:#f92672">.</span>log(E <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)) <span style="color:#66d9ef">for</span> E <span style="color:#f92672">in</span> E_list[::<span style="color:#ae81ff">10</span>]])
</code></pre></div><p><img src="/crf/inverse-response-after-each-iteration-3.png#center" alt="Inverse response after each iteration">
<img src="/crf/irradiance-image-after-each-iteration-3.png#center" alt="Irradiance image after each iteration"></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Normalized Ground-truth: &#34;</span>, normalize_255(get_extreme_sensor_irradiance()))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Normalized Estimate: &#34;</span>, normalize_255(E_list[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]))
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Error: &#34;</span>, normalize_255(
    get_extreme_sensor_irradiance()) <span style="color:#f92672">-</span> normalize_255(E_list[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]))
</code></pre></div>










  





  


<blockquote>
  <p><p>Normalized Ground-truth:  [  0.   5.  15. 255.]</p>
<p>Normalized Estimate:  [  0.   5.  15. 255.]</p>
<p>Error:  [0. 0. 0. 0.]</p>
</p>
  <footer>
    <strong></strong>
    
      
        
      
    
  </footer>
</blockquote>

<p>As we&rsquo;ll see in the next section, real datasets aren&rsquo;t quite as extreme and converge within a reasonable number of iterations.
In this example, we have only 2x2 images so any extreme values constitute a large percentage of the total pixels. Real-world
images might have larger extremes but are constrained to a smaller percentage of the image.</p>
<h2 id="calibrate-from-a-real-dataset-">Calibrate from a real dataset üéâ<a hidden class="anchor" aria-hidden="true" href="#calibrate-from-a-real-dataset-">#</a></h2>
<p>And finally, we get to test our algorithms on a non-simulated dataset acquired with an actual camera! Yay!</p>
<p>You can download the dataset that I&rsquo;ve used from here: <a href="https://vision.in.tum.de/data/datasets/mono-dataset">Monocular Visual Odometry
Dataset</a>. We&rsquo;re going to be using only two sequences from the
dataset: <em>narrow_sweep3</em> and <em>narrowGamma_sweep3</em>.</p>
<p>If you want to create a calibration dataset for your camera, you need to be able to programmatically control the
exposure time. The camera needs to be static and point towards a static scene with good variability in surface
reflectances. Ideally, you want to have close to a flat <a href="https://en.wikipedia.org/wiki/Image_histogram">histogram</a>. The
light source has to be constant ‚Äî beware that LED lights usually are not! You can use sunlight on a non-cloudy day.</p>
<p>Once you have that, you can fix the gain of your sensor to the lowest value to minimize noise and swipe the exposure
time (<span>
  \(t\)
</span>
) in small <span>
  \(\Delta t\)
</span>
 steps from the minimum value until the image becomes fully or mostly overexposed ‚Äî the
same way we&rsquo;ve done in our simulated datasets.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#f92672">import</span> os
<span style="color:#f92672">from</span> skimage <span style="color:#f92672">import</span> io
<span style="color:#f92672">from</span> skimage.measure <span style="color:#f92672">import</span> block_reduce

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_dataset</span>(path: str, with_downsample: bool <span style="color:#f92672">=</span> True,
                 block_size: Tuple[int, int] <span style="color:#f92672">=</span> (<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">10</span>)):
    <span style="color:#e6db74">&#34;&#34;&#34;Load dataset from the specified path
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    Subsambling the image size in order reduce the execution time
</span><span style="color:#e6db74">    of the (unoptimized) example algorithms
</span><span style="color:#e6db74">    
</span><span style="color:#e6db74">    path              -- path to the dataset
</span><span style="color:#e6db74">    with_downsample   -- enable/disable downsampling
</span><span style="color:#e6db74">    block_size        -- block size that gets sampled down to 1 pixel
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    image_folder <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(path, <span style="color:#e6db74">&#34;images&#34;</span>)
    exposure_file <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(path, <span style="color:#e6db74">&#34;times.txt&#34;</span>)
    gt_file <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(path, <span style="color:#e6db74">&#34;pcalib.txt&#34;</span>)

    exposures, images <span style="color:#f92672">=</span> [], []
    shape_original, shape_output <span style="color:#f92672">=</span> None, None
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">downsample</span>(image):
        <span style="color:#e6db74">&#34;&#34;&#34;Do max sampling in order to avoid bias from overexposed values&#34;&#34;&#34;</span>
        <span style="color:#66d9ef">return</span> block_reduce(image, block_size<span style="color:#f92672">=</span>block_size, func<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>max, cval<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
    
    <span style="color:#66d9ef">with</span> open(exposure_file, <span style="color:#e6db74">&#34;r&#34;</span>) <span style="color:#66d9ef">as</span> f:
        <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> f<span style="color:#f92672">.</span>readlines():
            <span style="color:#75715e"># Parse meta information and append to output lists</span>
            [idx, ts, exposure_ms] <span style="color:#f92672">=</span> line<span style="color:#f92672">.</span>strip()<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34; &#34;</span>)
            exposures<span style="color:#f92672">.</span>append(float(exposure_ms) <span style="color:#f92672">*</span> <span style="color:#ae81ff">1e-3</span>)
            
            <span style="color:#75715e"># Load and downsamble image</span>
            image_original <span style="color:#f92672">=</span> io<span style="color:#f92672">.</span>imread(os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(
                image_folder, idx <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;.jpg&#34;</span>), as_gray<span style="color:#f92672">=</span>True)
            image_output <span style="color:#f92672">=</span> downsample(image_original) \
                <span style="color:#66d9ef">if</span> with_downsample <span style="color:#66d9ef">else</span> image_original
            images<span style="color:#f92672">.</span>append(image_output<span style="color:#f92672">.</span>flatten())
  
            <span style="color:#66d9ef">if</span> shape_output <span style="color:#f92672">is</span> None:
                shape_original <span style="color:#f92672">=</span> image_original<span style="color:#f92672">.</span>shape
                shape_output <span style="color:#f92672">=</span> image_output<span style="color:#f92672">.</span>shape
    
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Original size: &#34;</span> <span style="color:#f92672">+</span> str(shape_original) <span style="color:#f92672">+</span> 
          <span style="color:#e6db74">&#34; Output size: &#34;</span> <span style="color:#f92672">+</span> str(shape_output))
    
    ground_truth <span style="color:#f92672">=</span> None
    <span style="color:#66d9ef">with</span> open(gt_file, <span style="color:#e6db74">&#39;r&#39;</span>) <span style="color:#66d9ef">as</span> f:
        ground_truth <span style="color:#f92672">=</span> [float(i) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> f<span style="color:#f92672">.</span>readline()<span style="color:#f92672">.</span>strip()<span style="color:#f92672">.</span>split()]
    
    <span style="color:#66d9ef">return</span> exposures, images, shape_output, ground_truth
</code></pre></div><p>Let&rsquo;s load the (downsampled) linear response dataset:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">exposures, images, shape, ground_truth <span style="color:#f92672">=</span> load_dataset(
    path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;data/calib_narrow_sweep3&#34;</span>)
</code></pre></div>










  





  


<blockquote>
  <p>Original size: (1024, 1280) Output size: (103, 128)</p>
  <footer>
    <strong></strong>
    
      
        
      
    
  </footer>
</blockquote>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">util<span style="color:#f92672">.</span>display_images(images[::<span style="color:#ae81ff">200</span>], shape<span style="color:#f92672">=</span>shape, annot<span style="color:#f92672">=</span>False)
</code></pre></div><p><img src="/crf/real-world-linear-dataset.png#center" alt="Real-world linear dataset"></p>
<p>I&rsquo;m going to use the calibration results provided with the <a href="https://vision.in.tum.de/data/datasets/mono-dataset">Monocular Visual Odometry
Dataset</a> as a reference ground-truth.</p>
<p><img src="/crf/linear-ground-truth.png#center" alt="Linear ground-truth"></p>
<p>Let&rsquo;s attempt to calibrate:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">U_list, E_list <span style="color:#f92672">=</span> recover_U_and_E(exposures, images, iterations<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>);
util<span style="color:#f92672">.</span>plot_multiple_inverse_crf(U_list)
util<span style="color:#f92672">.</span>display_images([normalize_255(np<span style="color:#f92672">.</span>log(E <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)) <span style="color:#66d9ef">for</span> E <span style="color:#f92672">in</span> E_list], 
                    shape<span style="color:#f92672">=</span>shape, annot<span style="color:#f92672">=</span>False)
</code></pre></div><p><img src="/crf/inverse-response-after-each-iteration-4.png#center" alt="Inverse response after each iteration">
<img src="/crf/irradiance-image-after-each-iteration-4.png#center" alt="Irradiance image after each iteration"></p>
<div class="alert alert-warning">
  <div class="alert-icon">üëâ</div>
  <div class="alert-content">Great, in just 3 iterations our algorithm converges to the correct solution!</div>
</div>

<p>Let&rsquo;s try it on the gamma encoded dataset as well.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">exposures, images, shape, ground_truth <span style="color:#f92672">=</span> load_dataset(
    path<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;data/calib_narrowGamma_sweep3&#34;</span>)
</code></pre></div>










  





  


<blockquote>
  <p>Original size: (1024, 1280) Output size: (103, 128)</p>
  <footer>
    <strong></strong>
    
      
        
      
    
  </footer>
</blockquote>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">util<span style="color:#f92672">.</span>display_images(images[::<span style="color:#ae81ff">200</span>], shape<span style="color:#f92672">=</span>shape, annot<span style="color:#f92672">=</span>False)
</code></pre></div><p><img src="/crf/real-world-gamma-dataset.png#gcenter" alt="Real-world gamma dataset">
<img src="/crf/gamma-ground-truth.png#center" alt="Linear ground-truth"></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">U_list, E_list <span style="color:#f92672">=</span> recover_U_and_E(exposures, images, iterations<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>);
util<span style="color:#f92672">.</span>plot_multiple_inverse_crf(U_list)
util<span style="color:#f92672">.</span>display_images([normalize_255(np<span style="color:#f92672">.</span>log(E <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>)) <span style="color:#66d9ef">for</span> E <span style="color:#f92672">in</span> E_list], 
                    shape<span style="color:#f92672">=</span>shape, annot<span style="color:#f92672">=</span>False)
</code></pre></div><p><img src="/crf/inverse-response-after-each-iteration-5.png#center" alt="Inverse response after each iteration">
<img src="/crf/irradiance-image-after-each-iteration-5.png#center" alt="Irradiance image after each iteration"></p>
<p>It is amazing to see how well it works even with 1/10 of the available information after subsampling the images from their original size of (1024, 1280) to (103, 128).</p>
<div class="alert alert-info">
  <div class="alert-icon">üí°</div>
  <div class="alert-content">Since we haven&rsquo;t enforced any constraints on the resulting (inverse) response function, we can calibrate for any non-linearity.</div>
</div>

<div class="alert alert-info">
  <div class="alert-icon">üí°</div>
  <div class="alert-content">An analogous approach can be used to calibrate RGB images. The process can be repeated for each of the R, G, B channels separately.</div>
</div>

<h1 id="final-words-">Final Words üèÅ<a hidden class="anchor" aria-hidden="true" href="#final-words-">#</a></h1>
<p>Although simple in its concept, the camera response function lies at the intersection of sensor physics and digital image processing. This makes it a fascinating topic to explore in-depth as it can lead our curiosity to a variety of unexplored paths.</p>
<p>We&rsquo;ve barely scratched the surface of camera sensor physics, simulation, noise models and applications of the CRF. This article should
hopefully provide a convenient starting point for self-learning and further research.</p>
<div class="alert alert-info">
  <div class="alert-icon">üí°</div>
  <div class="alert-content">Have a robotics topic in mind that is inadequately covered by existing learning resources?
Feel free to reach out to me on <a href="https://twitter.com/martinmihaylov_">üê¶ twitter</a> and it might be next. ;-)</div>
</div>

<script async data-uid="c95fcf1a9a" src="https://artisanal-pioneer-3052.ck.page/c95fcf1a9a/index.js"></script>


</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://roboalgorithms.com/tags/computer-vision/">computer vision</a></li>
      <li><a href="https://roboalgorithms.com/tags/python/">python</a></li>
    </ul>
  </footer>
</article>
    </main><footer class="footer">
    <span>&copy; 2021 <a href="https://roboalgorithms.com">RoboAlgorithms</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>


<script defer src="/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js" integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w="
    onload="hljs.initHighlightingOnLoad();"></script>
<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

</script>

</body>

</html>
